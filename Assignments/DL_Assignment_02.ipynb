{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "677c4d44",
      "metadata": {
        "id": "677c4d44"
      },
      "source": [
        "# 1.\tDescribe the structure of an artificial neuron. How is it similar to a biological neuron? What are its main components?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "513e2b81",
      "metadata": {
        "id": "513e2b81"
      },
      "source": [
        "ANS :  Artificial neural networks, like the human body's biological neural network, have a layered architecture and each network node (connection point) has the capability to process input and forward output to other nodes in the network. In both artificial and biological architectures, the nodes are called neurons and the connections are characterized by synaptic weights, \n",
        "        which represent the significance of the connection. As new data is received and processed,  the synaptic weights change and this is how learning occurs.   \n",
        "        Artificial neural networks, like the human body's biological neural network, \n",
        "        have a layered architecture and each network node (connection point) \n",
        "        has the capability to process input and forward output to other nodes in the network. \n",
        "        In both artificial and biological architectures, the nodes are called neurons and the connections are characterized by synaptic weights, which represent the significance of the connection. \n",
        "        As new data is received and processed, the synaptic weights change and this is how learning occurs.  \n",
        "    \n",
        "    There are three main components: an input later, a processing layer, and an output layer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb4ff971",
      "metadata": {
        "id": "cb4ff971"
      },
      "source": [
        "# 2.What are the different types of activation functions popularly used? Explain each of them."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14e1af92",
      "metadata": {
        "id": "14e1af92"
      },
      "source": [
        "ANS :The purpose of activation function is to add nonlinearity to the neural network \n",
        "    Different types of Activation functions popularly used are :\n",
        "         1:Binary Step Function :\n",
        "                    Binary step function depends on a threshold value that decides whether a neuron should be activated or not. \n",
        "                    The input fed to the activation function is compared to a certain threshold; if the input is greater than it, then the neuron is activated, else it is deactivated, meaning that its output is not passed on to the next hidden layer.\n",
        "             \n",
        "            Here are some of the limitations of binary step function:\n",
        "                1: It cannot provide multi-value outputs—for example, it cannot be used for multi-class classification problems. \n",
        "                2: The gradient of the step function is zero, which causes a hindrance in the backpropagation process.\n",
        "        2: Linear  Activation Function:\n",
        "                The linear activation function, also known as \"no activation,\" or \"identity function\" (multiplied x1.0), is where the activation is proportional to the input. \n",
        "                Mathematically given as f(x)=x\n",
        "                \n",
        "            However, a linear activation function has two major problems :\n",
        "                1: It’s not possible to use backpropagation as the derivative of the function is a constant and has no relation to the input x. \n",
        "                2: All layers of the neural network will collapse into one if a linear activation function is used. No matter the number of layers in the neural network, the last layer will still be a linear function of the first layer. So, essentially, a linear activation function turns the neural network into just one layer.\n",
        "                    \n",
        "        3: Non Linear Activation Function:\n",
        "                1: Sigmoid / Logistic Activation Function :\n",
        "                        This function takes any real value as input and outputs values in the range of 0 to 1. \n",
        "                        The larger the input (more positive), the closer the output value will be to 1.0, \n",
        "                        whereas the smaller the input (more negative), the closer the output will be to 0.0\n",
        "                         mathematically given as f(x)=1/(1+e^-x)\n",
        "                2: Hyperbolic tangent(Tanh)function:\n",
        "                        Tanh function is very similar to the sigmoid/logistic activation function, and even has the same S-shape with the difference in output range of -1 to 1. In Tanh, the larger the input (more positive), the closer the output value will be to 1.0, \n",
        "                        whereas the smaller the input (more negative), the closer the output will be to -1.0.\n",
        "                        \n",
        "                3:  ReLU Function:\n",
        "                        ReLU stands for Rectified Linear Unit. Although it gives an impression of a linear function, ReLU has a derivative function and allows for backpropagation while simultaneously making it computationally efficient. \n",
        "                        The main catch here is that the ReLU function does not activate all the neurons at the same time. \n",
        "                        The neurons will only be deactivated if the output of the linear transformation is less than 0.  \n",
        "                        \n",
        "                4:Leaky ReLU Function:\n",
        "                        Leaky ReLU is an improved version of ReLU function to solve the Dying ReLU problem as it has a small positive slope in the negative area.\n",
        "                        \n",
        "                5:Parametric ReLU Function:\n",
        "                        Parametric ReLU is another variant of ReLU that aims to solve the problem of gradient’s becoming zero for the left half of the axis. \n",
        "                        This function provides the slope of the negative part of the function as an argument a. By performing backpropagation, the most appropriate value of a is learnt."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af701170",
      "metadata": {
        "id": "af701170"
      },
      "source": [
        "# 3.\t\n",
        "1.\tExplain, in details, Rosenblatt’s perceptron model. How can a set of data be classified using a simple perceptron?\n",
        "2.\tUse a simple perceptron with weights w0, w1, and w2 as −1, 2, and 1, respectively, to classify data points (3, 4); (5, 2); (1, −3); (−8, −3); (−3, 0).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3322ba1",
      "metadata": {
        "id": "a3322ba1"
      },
      "source": [
        "ANS:\n",
        "    Rosenblatt perceptron is a binary single neuron model. The inputs integration is implemented through the addition of the weighted inputs that have fixed weights obtained during the training stage. If the result of this addition is larger than a given threshold θ the neuron fires. When the neuron fires its output is set to 1, otherwise it’s set to 0.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c66db00c",
      "metadata": {
        "id": "c66db00c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d0f5061f",
      "metadata": {
        "id": "d0f5061f"
      },
      "source": [
        "# 4.\tExplain the basic structure of a multi-layer perceptron. Explain how it can solve the XOR problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69f63b8c",
      "metadata": {
        "id": "69f63b8c"
      },
      "source": [
        "ANS:Deep Learning deals with training multi-layer artificial neural networks, also called Deep Neural Networks. After Rosenblatt perceptron was developed in the 1950s, there was a lack of interest in neural networks until 1986, when Dr.Hinton and his colleagues developed the backpropagation algorithm to train a multilayer neural network.\n",
        "    A fully connected multi-layer neural network is called a Multilayer Perceptron (MLP).The XOr problem is that we need to build a Neural Network (a perceptron in our case) to produce the truth table related to the XOr logical operator. This is a binary classification problem. Hence, supervised learning is a better way to solve it. In this case, we will be using perceptrons. Uni layered perceptrons can only work with linearly separable data.\n",
        "    To solve this problem, we add an extra layer to our vanilla perceptron, i.e., we create a Multi Layered Perceptron (or MLP). We call this extra layer as the Hidden layer. To build a perceptron, we first need to understand that the XOr gate can be written as a combination of AND gates, NOT gates and OR gates in the following way:\n",
        "a XOr b = (a AND NOT b)OR(bAND NOTa)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30346ea3",
      "metadata": {
        "id": "30346ea3"
      },
      "source": [
        "# 5.\tWhat is artificial neural network (ANN)? Explain some of the salient highlights in the different architectural options for ANN."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "163b9cc1",
      "metadata": {
        "id": "163b9cc1"
      },
      "source": [
        "ANS :An Artificial Neural Network (ANN) is an information processing paradigm that is inspired by the brain. ANNs, like people, learn by examples. An ANN is configured for a specific application, such as pattern recognition or data classification, through a learning process. Learning largely involves adjustments to the synaptic connections that exist between the neurons. \n",
        "\n",
        "Artificial Neural Networks (ANNs) are a type of machine learning model that are inspired by the structure and function of the human brain. They consist of layers of interconnected “neurons” that process and transmit information.\n",
        "\n",
        "There are several different architectures for ANNs, each with their own strengths and weaknesses. Some of the most common architectures include:\n",
        "    \n",
        "    \n",
        "    Feedforward Neural Networks: This is the simplest type of ANN architecture, where the information flows in one direction from input to output. The layers are fully connected, meaning each neuron in a layer is connected to all the neurons in the next layer.\n",
        "\n",
        "Recurrent Neural Networks (RNNs): These networks have a “memory” component, where information can flow in cycles through the network. This allows the network to process sequences of data, such as time series or speech.\n",
        "\n",
        "Convolutional Neural Networks (CNNs): These networks are designed to process data with a grid-like topology, such as images. The layers consist of convolutional layers, which learn to detect specific features in the data, and pooling layers, which reduce the spatial dimensions of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8c6ca97",
      "metadata": {
        "id": "d8c6ca97"
      },
      "source": [
        "# 6.\tExplain the learning process of an ANN. Explain, with example, the challenge in assigning synaptic weights for the interconnection between neurons? How can this challenge be addressed?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "621178b4",
      "metadata": {
        "id": "621178b4"
      },
      "source": [
        "ANS :\n",
        "    Basically, learning means to do and adapt the change in itself as and when there is a change in environment. ANN is a complex system or more precisely we can say that it is a complex adaptive system, which can change its internal structure based on the information passing through it.\n",
        "    We know that, during ANN learning, to change the input/output behavior, we need to adjust the weights. Hence, a method is required with the help of which the weights can be modified. These methods are called Learning rules, which are simply algorithms or equations. Following are some learning rules for the neural network −\n",
        "Hebbian Learning Rule\n",
        "\n",
        "This rule, one of the oldest and simplest, was introduced by Donald Hebb in his book The Organization of Behavior in 1949. It is a kind of feed-forward, unsupervised learning.\n",
        "\n",
        "Basic Concept − This rule is based on a proposal given by Hebb, who wrote −\n",
        "\n",
        "“When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased.”\n",
        "    \n",
        "      Mathematical Formulation − According to Hebbian learning rule, following is the formula to increase the weight of connection at every time step.\n",
        "\n",
        "    Δwji(t)=αxi(t).yj(t)\n",
        "    Here, Δwji(t)⁡= increment by which the weight of connection increases at time step t\n",
        "\n",
        "    α= the positive and constant learning rate\n",
        "\n",
        "    xi(t)= the input value from pre-synaptic neuron at time step t\n",
        "\n",
        "    yi(t)= the output of pre-synaptic neuron at same time step t\n",
        "    \n",
        "    ------------------------------------------------------------------------\n",
        "    Perceptron Learning Rule\n",
        "\n",
        "This rule is an error correcting the supervised learning algorithm of single layer feedforward networks with linear activation function, introduced by Rosenblatt.\n",
        "\n",
        "Basic Concept − As being supervised in nature, to calculate the error, there would be a comparison between the desired/target output and the actual output. If there is any difference found, then a change must be made to the weights of connection.\n",
        "\n",
        "Mathematical Formulation − To explain its mathematical formulation, suppose we have ‘n’ number of finite input vectors, xn\n",
        ", along with its desired/target output vector tn\n",
        "\n",
        ", where n = 1 to N.\n",
        "\n",
        "Now the output ‘y’ can be calculated, as explained earlier on the basis of the net input, and activation function being applied over that net input can be expressed as follows −\n",
        "\n",
        "y=f(yin)={1,0,yin>θyin⩽θ\n",
        "\n",
        "Where θ is threshold.\n",
        "\n",
        "The updating of weight can be done in the following two cases −\n",
        "\n",
        "Case I − when t ≠ y, then\n",
        "\n",
        "w(new)=w(old)+tx\n",
        "\n",
        "Case II − when t = y, then\n",
        "\n",
        "No change in weight\n",
        "          ------------------------------------\n",
        "          \n",
        "          \n",
        "Delta Learning Rule Widrow−HoffRule\n",
        "\n",
        "It is introduced by Bernard Widrow and Marcian Hoff, also called Least Mean Square LMS\n",
        "\n",
        "method, to minimize the error over all training patterns. It is kind of supervised learning algorithm with having continuous activation function.\n",
        "\n",
        "Basic Concept − The base of this rule is gradient-descent approach, which continues forever. Delta rule updates the synaptic weights so as to minimize the net input to the output unit and the target value.\n",
        "\n",
        "Mathematical Formulation − To update the synaptic weights, delta rule is given by\n",
        "\n",
        "Δwi=α.xi.ej\n",
        "\n",
        "Here Δwi= weight change for ith ⁡pattern;\n",
        "          \n",
        "          ---------------------------------\n",
        "\n",
        "Competitive Learning Rule Winner−takes−all\n",
        "\n",
        "It is concerned with unsupervised training in which the output nodes try to compete with each other to represent the input pattern. To understand this learning rule, we must understand the competitive network which is given as follows −\n",
        "\n",
        "Basic Concept of Competitive Network − This network is just like a single layer feedforward network with feedback connection between outputs. The connections between outputs are inhibitory type, shown by dotted lines, which means the competitors never support themselves.\n",
        "          \n",
        "          \n",
        "\n",
        "\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80860fe9",
      "metadata": {
        "id": "80860fe9"
      },
      "source": [
        "# 7.\tExplain, in details, the backpropagation algorithm. What are the limitations of this algorithm?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8c192dd",
      "metadata": {
        "id": "f8c192dd"
      },
      "source": [
        "ANS :Backpropagation is an algorithm that backpropagates the errors from the output nodes to the input nodes. Therefore, it is simply referred to as the backward propagation of errors. It uses in the vast applications of neural networks in data mining like Character recognition, Signature verification, etc.\n",
        "    Backpropagation is a widely used algorithm for training feedforward neural networks. It computes the gradient of the loss function with respect to the network weights. It is very efficient, rather than naively directly computing the gradient concerning each weight. This efficiency makes it possible to use gradient methods to train multi-layer networks and update weights to minimize loss; variants such as gradient descent or stochastic gradient descent are often used.\n",
        "\n",
        "The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight via the chain rule, computing the gradient layer by layer, and iterating backward from the last layer to avoid redundant computation of intermediate terms in the chain rule.\n",
        "\n",
        "Backpropagation Algorithm:\n",
        "\n",
        "Step 1: Inputs X, arrive through the preconnected path.\n",
        "\n",
        "Step 2: The input is modeled using true weights W. Weights are usually chosen randomly.\n",
        "Step 3: Calculate the output of each neuron from the input layer to the hidden layer to the output layer.\n",
        "\n",
        "Step 4: Calculate the error in the outputs\n",
        "\n",
        "Backpropagation Error= Actual Output – Desired Output\n",
        "\n",
        "Step 5: From the output layer, go back to the hidden layer to adjust the weights to reduce the error.\n",
        "\n",
        "Step 6: Repeat the process until the desired output is achieved.\n",
        "    Parameters :\n",
        "\n",
        "    x = inputs training vector x=(x1,x2,…………xn).\n",
        "    t = target vector t=(t1,t2……………tn).\n",
        "    δk = error at output unit.\n",
        "    δj  = error at hidden layer.\n",
        "    α = learning rate.\n",
        "    V0j = bias of hidden unit j.\n",
        "    \n",
        "Training Algorithm :\n",
        "\n",
        "Step 1: Initialize weight to small random values.\n",
        "\n",
        "Step 2: While the stepsstopping condition is to be false do step 3 to 10.\n",
        "\n",
        "Step 3: For each training pair do step 4 to 9 (Feed-Forward).\n",
        "\n",
        "Step 4: Each input unit receives the signal unit and transmitsthe signal xi signal to all the units.\n",
        "\n",
        "Step 5 : Each hidden unit Zj (z=1 to a) sums its weighted input signal to calculate its net input \n",
        "\n",
        "                     zinj = v0j + Σxivij     ( i=1 to n)\n",
        "\n",
        "           Applying activation function zj = f(zinj) and sends this signals to all units in the layer about i.e output units\n",
        "\n",
        "           For each output l=unit yk = (k=1 to m) sums its weighted input signals.\n",
        "\n",
        "                     yink = w0k + Σ ziwjk    (j=1 to a)\n",
        "\n",
        "           and applies its activation function to calculate the output signals.\n",
        "\n",
        "                     yk = f(yink)\n",
        "            \n",
        "Disadvantages:\n",
        "\n",
        "    It is sensitive to noisy data and irregularities. Noisy data can lead to inaccurate results.\n",
        "    Performance is highly dependent on input data.\n",
        "    Spending too much time training.\n",
        "    The matrix-based approach is preferred over a mini-batch."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "116b1a71",
      "metadata": {
        "id": "116b1a71"
      },
      "source": [
        "# 8.\tDescribe, in details, the process of adjusting the interconnection weights in a multi-layer neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS :The process of adjusting the interconnection weights in a multi-layer neural network is called training. The goal of training is to find the optimal set of weights that minimize the prediction error of the network. This is done by using a loss function that measures the difference between the network's predictions and the actual target outputs.\n",
        "\n",
        "The process of adjusting the weights in a neural network is based on gradient descent, an optimization algorithm that minimizes the loss function. The gradient of the loss function with respect to the weights provides information on how to adjust the weights in order to reduce the loss.\n",
        "\n",
        "The training process consists of the following steps:\n",
        "\n",
        "    Feed-forward: The input data is passed through the network, and the outputs of each layer are computed by applying the activation function to the weighted sum of the inputs. This results in the final prediction of the network.\n",
        "\n",
        "    Computation of the loss: The difference between the network's predictions and the actual target outputs is computed using a loss function such as mean squared error or categorical cross-entropy.\n",
        "\n",
        "    Backpropagation: The gradient of the loss with respect to the weights is computed by applying the chain rule of differentiation. The gradients are propagated backwards through the network, starting from the output layer and proceeding layer by layer towards the input layer.\n",
        "\n",
        "    Weight adjustment: The weights are updated in the direction of the negative gradient using an optimization algorithm such as stochastic gradient descent (SGD), Adam, or RProp. The update step is determined by the learning rate, which determines the magnitude of the weight adjustment.\n",
        "\n",
        "    Repeat: The feed-forward, computation of the loss, backpropagation, and weight adjustment steps are repeated for a specified number of epochs or until a stopping criterion is met.\n",
        "\n",
        "This process is repeated until the loss function converges to a minimum, and the weights of the network are optimized to minimize the prediction error. The training process is a critical step in the development of a neural network and requires careful tuning of the hyperparameters, such as the learning rate, the number of hidden layers, and the number of neurons in each layer."
      ],
      "metadata": {
        "id": "K4E_omrycj9C"
      },
      "id": "K4E_omrycj9C"
    },
    {
      "cell_type": "markdown",
      "id": "67991d94",
      "metadata": {
        "id": "67991d94"
      },
      "source": [
        "# 9.\tWhat are the steps in the backpropagation algorithm? Why a multi-layer neural network is required?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1349a2a",
      "metadata": {
        "id": "b1349a2a"
      },
      "source": [
        "ANS :Backpropagation is an algorithm that backpropagates the errors from the output nodes to the input nodes. Therefore, it is simply referred to as the backward propagation of errors. It uses in the vast applications of neural networks in data mining like Character recognition, Signature verification, etc.\n",
        "    Working of Backpropagation:\n",
        "\n",
        "Neural networks use supervised learning to generate output vectors from input vectors that the network operates on. It Compares generated output to the desired output and generates an error report if the result does not match the generated output vector. Then it adjusts the weights according to the bug report to get your desired output.\n",
        "Backpropagation Algorithm:\n",
        "\n",
        "Step 1: Inputs X, arrive through the preconnected path.\n",
        "\n",
        "Step 2: The input is modeled using true weights W. Weights are usually chosen randomly.\n",
        "\n",
        "Step 3: Calculate the output of each neuron from the input layer to the hidden layer to the output layer.\n",
        "\n",
        "Step 4: Calculate the error in the outputs\n",
        "\n",
        "Backpropagation Error= Actual Output – Desired Output\n",
        "\n",
        "Step 5: From the output layer, go back to the hidden layer to adjust the weights to reduce the error.\n",
        "\n",
        "Step 6: Repeat the process until the desired output is achieved."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40b2a1ee",
      "metadata": {
        "id": "40b2a1ee"
      },
      "source": [
        "# 10.\tWrite short notes on:\n",
        "1.\tArtificial neuron\n",
        "2.\tMulti-layer perceptron\n",
        "3.\tDeep learning\n",
        "4.\tLearning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a06d480b",
      "metadata": {
        "id": "a06d480b"
      },
      "source": [
        "ANS :\n",
        "    Artificial Neuron: \n",
        "        The term \"Artificial neural network\" refers to a biologically inspired sub-field of artificial intelligence modeled after the brain. An Artificial neural network is usually a computational network based on biological neural networks that construct the structure of the human brain. Similar to a human brain has neurons interconnected to each other, artificial neural networks also have neurons that are linked to each other in various layers of the networks. These neurons are known as nodes.\n",
        "        Input Layer:As the name suggests, it accepts inputs in several different formats provided by the programmer.\n",
        "        Hidden Layer:The hidden layer presents in-between input and output layers. It performs all the calculations to find hidden features and patterns.\n",
        "        Output Layer:The input goes through a series of transformations using the hidden layer, which finally results in output that is conveyed using this layer.\n",
        "        The artificial neural network takes input and computes the weighted sum of the inputs and includes a bias. This computation is represented in the form of a transfer function.\n",
        "        \n",
        "        \n",
        "    MultiLayer perceptron:\n",
        "        A multilayer perceptron is a neural network connecting multiple layers in a directed graph, which means that the signal path through the nodes only goes one way. Each node, apart from the input nodes, has a nonlinear activation function. An MLP uses backpropagation as a supervised learning technique. Since there are multiple layers of neurons, MLP is a deep learning technique.\n",
        "        MLP is widely used for solving problems that require supervised learning as well as research into computational neuroscience and parallel distributed processing. Applications include speech recognition, image recognition and machine translation. \n",
        "        \n",
        "    Deep Learning:\n",
        "        Deep learning is a subset of machine learning, which is essentially a neural network with three or more layers. These neural networks attempt to simulate the behavior of the human brain—albeit far from matching its ability—allowing it to “learn” from large amounts of data. While a neural network with a single layer can still make approximate predictions, additional hidden layers can help to optimize and refine for accuracy.\n",
        "        Deep learning drives many artificial intelligence (AI) applications and services that improve automation, performing analytical and physical tasks without human intervention. Deep learning technology lies behind everyday products and services (such as digital assistants, voice-enabled TV remotes, and credit card fraud detection) as well as emerging technologies (such as self-driving cars).\n",
        "        Deep learning eliminates some of data pre-processing that is typically involved with machine learning. These algorithms can ingest and process unstructured data, like text and images, and it automates feature extraction, removing some of the dependency on human experts. For example, let’s say that we had a set of photos of different pets, and we wanted to categorize by “cat”, “dog”, “hamster”, et cetera. Deep learning algorithms can determine which features (e.g. ears) are most important to distinguish each animal from another. In machine learning, this hierarchy of features is established manually by a human expert.\n",
        "        Then, through the processes of gradient descent and backpropagation, the deep learning algorithm adjusts and fits itself for accuracy, allowing it to make predictions about a new photo of an animal with increased precision.  \n",
        "\n",
        "    Learning Rate:\n",
        "        Learning rate is a hyper-parameter that controls how much we are adjusting the weights of our network with respect the loss gradient. The lower the value, the slower we travel along the downward slope. While this might be a good idea (using a low learning rate) in terms of making sure that we do not miss any local minima, it could also mean that we’ll be taking a long time to converge — especially if we get stuck on a plateau region.\n",
        "        new_weight = existing_weight — learning_rate * gradient\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d407cec",
      "metadata": {
        "id": "1d407cec"
      },
      "source": [
        "# 11.\tWrite the difference between:-\n",
        "1.\tActivation function vs threshold function\n",
        "2.\tStep function vs sigmoid function\n",
        "3.\tSingle layer vs multi-layer perceptron\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0be7c12b",
      "metadata": {
        "id": "0be7c12b"
      },
      "source": [
        "ANS :\n",
        "    1: Activation function vs threshold function --->A threshold value determines whether a neuron should be activated or not activated in a binary step activation function. The activation function compares the input value to a threshold value. If the input value is greater than the threshold value, the neuron is activated\n",
        "        \n",
        "    2: Step function vs sigmoid function:\n",
        "            The (Heaviside) step function is typically only useful within single-layer perceptrons, an early type of neural networks that can be used for classification in cases where the input data is linearly separable.\n",
        "            However, multi-layer neural networks or multi-layer perceptrons are of more interest because they are general function approximators and they are able to distinguish data that is not linearly separable.\n",
        "            Multi-layer perceptrons are trained using backpropapagation. A requirement for backpropagation is a differentiable activation function. That's because backpropagation uses gradient descent on this function to update the network weights.\n",
        "            The Heaviside step function is non-differentiable at x = 0 and its derivative is 0 elsewhere. This means gradient descent won't be able to make progress in updating the weights and backpropagation will fail.\n",
        "             The sigmoid or logistic function does not have this shortcoming and this explains its usefulness as an activation function within the field of neural networks.\n",
        "    \n",
        "    3:Single layer vs multi layer perceptron:\n",
        "            Perceptron is a neural network with only one neuron, and can only understand linear relationships between the input and output data provided. However, with Multilayer Perceptron, horizons are expanded and now this neural network can have many layers of neurons, and ready to learn more complex patterns\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04143243",
      "metadata": {
        "id": "04143243"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}