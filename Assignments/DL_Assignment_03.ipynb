{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Is it OK to initialize all the weights to the same value as long as that value is selected\n",
        "randomly using He initialization?"
      ],
      "metadata": {
        "id": "eefji3T4VkgT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS :\n",
        "    Now imagine that you initialize all weights to the same value (e.g. zero or one). In this case, each hidden unit will get exactly the same signal. E.g. if all weights are initialized to 1, each unit gets signal equal to sum of inputs (and outputs sigmoid(sum(inputs)) ).\n",
        "    The weights attached to the same neuron, continue to remain the same throughout the training. It makes the hidden units symmetric and this problem is known as the symmetry problem. Hence to break this symmetry the weights connected to the same neuron should not be initialized to the same value"
      ],
      "metadata": {
        "id": "_VI4SkBzVov1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Is it OK to initialize the bias terms to 0?\n",
        "\n",
        "ANS :It is important to note that setting biases to 0 will not create any problems as non-zero weights take care of breaking the symmetry and even if bias is 0, the values in every neuron will still be different."
      ],
      "metadata": {
        "id": "lt_t-a87V8O5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Name three advantages of the SELU activation function over ReLU.\n",
        "\n",
        "\n",
        "ANS :: \n",
        "    1:   Like ReLU, SELU does not have vanishing gradient problem and hence, is used in deep neural networks.\n",
        "    2: Compared to ReLUs, SELUs cannot die.\n",
        "    3: SELUs learn faster and better than other activation functions without needing further procession."
      ],
      "metadata": {
        "id": "vimR3I07WIWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. In which cases would you want to use each of the following activation functions: SELU, leaky\n",
        "ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
        "\n",
        "\n",
        "ANS ::\n",
        "\n",
        "    1:Sigmoid function\n",
        "     Advatange : it is a non linear function \n",
        "                 it is differentiable function \n",
        "                 it is used for bianry classification problem \n",
        "      Disadvantage \n",
        "                 it is saturating function (vanishing gradient problem)\n",
        "                 non zero centered\n",
        "                 computationally expensive\n",
        "\n",
        "    2: Tanh :  \n",
        "      it is just like sigmoid but only diffference is that it lies between -1 to 1 rather than 0 to 1\n",
        "      Advantages:\n",
        "       1. it is non linear function\n",
        "       2. it is differentiable \n",
        "       3. it is 0 centered \n",
        "\n",
        "       Disadvantage:    \n",
        "       1.it is saturating function (vanishing gradient problem)\n",
        "       2. computationally expensive \n",
        "     \n",
        "    3: Relu   :    \n",
        "       Advantage :    \n",
        "        1. it is non linear \n",
        "        2. it is coputationaly inexpensive \n",
        "        3. convergence is faster \n",
        "       Disadvantage  :    \n",
        "       1. it is not 0 centered \n",
        "       2. it is not fully differentiable \n",
        "    4: Variants of Relu\n",
        "     1. Linear \n",
        "     2 .Non Linear \n",
        "      \n",
        "      Linear :   Leaky Relu and Parametric relu \n",
        "      Non Linear :   Elu and Selu \n",
        "\n",
        "    Leaky Relu ::: \n",
        "       f(x)=max(0.01Z,Z) means for value grater than 0 it is Z and less than 0 it is 0.01Z\n",
        "       Advantage :   1.Never dying relu problem\n",
        "                     2. non saturating \n",
        "                     3. computationally inexpensive \n",
        "    Parametric Relu :    \n",
        "      same as leaky relu expect 0.01  some parametric value is passed \n",
        "\n",
        "    Exponential Relu :     \n",
        "      f(x) =z for positive and @(e^z-1) for negative where @ is constant \n",
        "    \n",
        "    softmax :    \n",
        "    The Softmax function is a commonly used activation function in deep learning, especially in the output layer of multi-class classification problems. It transforms the input vector into a probability distribution over multiple classes.Given a vector of real numbers x, the softmax function maps each element of the input vector to a value between 0 and 1, such that the sum of all elements of the output vector is equal to 1. This makes the output of the softmax function a valid probability distribution, representing the predicted class probabilities for a given input.\n",
        "\n",
        "  The softmax function is defined as:\n",
        "\n",
        "  softmax(x_i) = e^(x_i) / sum(e^(x_j)) for all j\n",
        "\n",
        "  where x_i is the i-th element of the input vector x, and e is the base of the natural logarithm. The softmax function provides a smooth, continuous and differentiable activation, making it suitable for optimization using gradient-based methods.\n",
        " In practice, the softmax function is used as the final activation function in the output layer of neural networks for multi-class classification problems, where the predicted class is the one with the highest probability. The predicted class probabilities can be used for various applications, such as calculating the cross-entropy loss for training the network or for computing performance metrics such as accuracy, precision, recall, and F1 score\n",
        "\n",
        " SELU :\n",
        "  Scaled Exponential Linear Unit (SELU) is an activation function that was introduced in a paper by Klambauer et al. (2017) as an alternative to traditional activation functions such as ReLU and sigmoid.\n",
        "\n",
        "SELU is defined as:\n",
        "\n",
        "f(x) = lambda * x if x > 0\n",
        "lambda * alpha * (exp(x) - 1) if x <= 0\n",
        "\n",
        "where lambda and alpha are two scaling factors that are chosen such that the mean and variance of the activations in a deep network with SELU activations remain constant, even for very deep networks. This property, known as self-normalization, can lead to faster convergence of the network compared to traditional activation functions.\n",
        "\n",
        "SELU is designed to be used in feedforward networks with a special weight initialization technique that ensures that the activations of the neurons are transformed into a desired distribution. When used in combination with this weight initialization technique, SELUs have been shown to outperform traditional activation functions in deep networks.\n",
        "\n",
        "In practice, SELU is used as a activation function in feedforward neural networks, and it has been used in various applications, such as image classification, natural language processing, and reinforcement learning. However, SELU is not as widely used as other activation functions such as ReLU and sigmoid, and the choice of activation function ultimately depends on the specific problem and data.\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "Wr99irjEWg3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999)\n",
        "when using an SGD optimizer?\n",
        "\n",
        "ANS :If you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer, then the algorithm will likely pick up a lot of speed, hopefully moving roughly toward the global minimum, but its momentum will carry it right past the minimum"
      ],
      "metadata": {
        "id": "dJ46_Z6jWnPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Name three ways you can produce a sparse model.\n",
        "\n",
        "ANS :Three ways to produce a sparse model are:\n",
        "\n",
        "    Regularization techniques: Regularization techniques such as L1 regularization and Lasso regularization add a penalty term to the loss function that discourages the model from using too many parameters. This results in sparse models with only a few parameters that are most relevant for the task.\n",
        "\n",
        "    Pruning: Pruning is a technique that removes redundant or less important neurons and connections in a neural network. This can lead to sparse models that are more interpretable and computationally efficient.\n",
        "\n",
        "    Early Stopping: Early stopping is a technique where training is stopped before the model has a chance to overfit the data. Overfitting can lead to models with many parameters that are not relevant for the task, and early stopping can prevent this, resulting in sparse models.\n",
        "\n",
        "These methods can be used together or individually to produce sparse models, depending on the specific problem and the desired level of sparsity. The choice of method depends on the problem, the data, and the goals of the modeling."
      ],
      "metadata": {
        "id": "Qr5HCnizWu1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on\n",
        "new instances)? What about MC Dropout?\n",
        "\n",
        "\n",
        "ANS :Dropout boils down to simply switching-off some neurons at each training step. At each step, a different set of neurons are switched off. Mathematically speaking, each neuron has some probability p of being ignored, called the dropout rate. The dropout rate is typically set to be between 0 (no dropout) and 0.5 (approximately 50% of all neurons will be switched off). The exact value depends on the network type, layer size, and the degree to which the network overfits the training data.\n",
        "Dropout is a regularization technique, that is, it helps prevent overfitting. With little data and/or a complex network, the model might memorize the training data and, as a result, work great on the data it has seen during training but deliver terrible results on new, unseen data. This is called overfitting, and dropout seeks to alleviate it.\n",
        "Dropout is a regularization technique, that is, it helps prevent overfitting. With little data and/or a complex network, the model might memorize the training data and, as a result, work great on the data it has seen during training but deliver terrible results on new, unseen data. This is called overfitting, and dropout seeks to alleviate it.\n",
        "dropout is only used during training. At inference time, that is when we make predictions with our network, we typically don’t apply any dropout — we want to use all the trained neurons and connections."
      ],
      "metadata": {
        "id": "xxupjCbyWvJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
        "\n",
        "\n",
        "a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the\n",
        "point of this exercise). Use He initialization and the ELU activation function.\n",
        "\n",
        "\n",
        "b. Using Nadam optimization and early stopping, train the network on the CIFAR10\n",
        "dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is\n",
        "composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for\n",
        "testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons.\n",
        "Remember to search for the right learning rate each time you change the model’s\n",
        "architecture or hyperparameters.\n",
        "\n",
        "\n",
        "c. Now try adding Batch Normalization and compare the learning curves: Is it\n",
        "converging faster than before? Does it produce a better model? How does it affect\n",
        "training speed?\n",
        "\n",
        "\n",
        "d. Try replacing Batch Normalization with SELU, and make the necessary adjustements\n",
        "to ensure the network self-normalizes (i.e., standardize the input features, use\n",
        "LeCun normal initialization, make sure the DNN contains only a sequence of dense\n",
        "layers, etc.).\n",
        "\n",
        "\n",
        "e. Try regularizing the model with alpha dropout. Then, without retraining your model,\n",
        "see if you can achieve better accuracy using MC Dropout."
      ],
      "metadata": {
        "id": "_pYSxv8HWvM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "9BTmhk2KWGvp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10=tf.keras.datasets.cifar10"
      ],
      "metadata": {
        "id": "oP_mmetdfXc6"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJWHP6pbfiAC",
        "outputId": "733d2f1d-da87-4aeb-ef3b-561bf2ba219e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'keras.api._v2.keras.datasets.cifar10' from '/usr/local/lib/python3.8/dist-packages/keras/api/_v2/keras/datasets/cifar10/__init__.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train,y_train),(X_test,y_test)=cifar10.load_data()"
      ],
      "metadata": {
        "id": "MjK998kRf694"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zhs7JBN2gGtQ",
        "outputId": "8c4e081b-cd33-4ec5-a00f-8d094133aeda"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[ 59,  62,  63],\n",
              "         [ 43,  46,  45],\n",
              "         [ 50,  48,  43],\n",
              "         ...,\n",
              "         [158, 132, 108],\n",
              "         [152, 125, 102],\n",
              "         [148, 124, 103]],\n",
              "\n",
              "        [[ 16,  20,  20],\n",
              "         [  0,   0,   0],\n",
              "         [ 18,   8,   0],\n",
              "         ...,\n",
              "         [123,  88,  55],\n",
              "         [119,  83,  50],\n",
              "         [122,  87,  57]],\n",
              "\n",
              "        [[ 25,  24,  21],\n",
              "         [ 16,   7,   0],\n",
              "         [ 49,  27,   8],\n",
              "         ...,\n",
              "         [118,  84,  50],\n",
              "         [120,  84,  50],\n",
              "         [109,  73,  42]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[208, 170,  96],\n",
              "         [201, 153,  34],\n",
              "         [198, 161,  26],\n",
              "         ...,\n",
              "         [160, 133,  70],\n",
              "         [ 56,  31,   7],\n",
              "         [ 53,  34,  20]],\n",
              "\n",
              "        [[180, 139,  96],\n",
              "         [173, 123,  42],\n",
              "         [186, 144,  30],\n",
              "         ...,\n",
              "         [184, 148,  94],\n",
              "         [ 97,  62,  34],\n",
              "         [ 83,  53,  34]],\n",
              "\n",
              "        [[177, 144, 116],\n",
              "         [168, 129,  94],\n",
              "         [179, 142,  87],\n",
              "         ...,\n",
              "         [216, 184, 140],\n",
              "         [151, 118,  84],\n",
              "         [123,  92,  72]]],\n",
              "\n",
              "\n",
              "       [[[154, 177, 187],\n",
              "         [126, 137, 136],\n",
              "         [105, 104,  95],\n",
              "         ...,\n",
              "         [ 91,  95,  71],\n",
              "         [ 87,  90,  71],\n",
              "         [ 79,  81,  70]],\n",
              "\n",
              "        [[140, 160, 169],\n",
              "         [145, 153, 154],\n",
              "         [125, 125, 118],\n",
              "         ...,\n",
              "         [ 96,  99,  78],\n",
              "         [ 77,  80,  62],\n",
              "         [ 71,  73,  61]],\n",
              "\n",
              "        [[140, 155, 164],\n",
              "         [139, 146, 149],\n",
              "         [115, 115, 112],\n",
              "         ...,\n",
              "         [ 79,  82,  64],\n",
              "         [ 68,  70,  55],\n",
              "         [ 67,  69,  55]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[175, 167, 166],\n",
              "         [156, 154, 160],\n",
              "         [154, 160, 170],\n",
              "         ...,\n",
              "         [ 42,  34,  36],\n",
              "         [ 61,  53,  57],\n",
              "         [ 93,  83,  91]],\n",
              "\n",
              "        [[165, 154, 128],\n",
              "         [156, 152, 130],\n",
              "         [159, 161, 142],\n",
              "         ...,\n",
              "         [103,  93,  96],\n",
              "         [123, 114, 120],\n",
              "         [131, 121, 131]],\n",
              "\n",
              "        [[163, 148, 120],\n",
              "         [158, 148, 122],\n",
              "         [163, 156, 133],\n",
              "         ...,\n",
              "         [143, 133, 139],\n",
              "         [143, 134, 142],\n",
              "         [143, 133, 144]]],\n",
              "\n",
              "\n",
              "       [[[255, 255, 255],\n",
              "         [253, 253, 253],\n",
              "         [253, 253, 253],\n",
              "         ...,\n",
              "         [253, 253, 253],\n",
              "         [253, 253, 253],\n",
              "         [253, 253, 253]],\n",
              "\n",
              "        [[255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         ...,\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255],\n",
              "         [255, 255, 255]],\n",
              "\n",
              "        [[255, 255, 255],\n",
              "         [254, 254, 254],\n",
              "         [254, 254, 254],\n",
              "         ...,\n",
              "         [254, 254, 254],\n",
              "         [254, 254, 254],\n",
              "         [254, 254, 254]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[113, 120, 112],\n",
              "         [111, 118, 111],\n",
              "         [105, 112, 106],\n",
              "         ...,\n",
              "         [ 72,  81,  80],\n",
              "         [ 72,  80,  79],\n",
              "         [ 72,  80,  79]],\n",
              "\n",
              "        [[111, 118, 110],\n",
              "         [104, 111, 104],\n",
              "         [ 99, 106,  98],\n",
              "         ...,\n",
              "         [ 68,  75,  73],\n",
              "         [ 70,  76,  75],\n",
              "         [ 78,  84,  82]],\n",
              "\n",
              "        [[106, 113, 105],\n",
              "         [ 99, 106,  98],\n",
              "         [ 95, 102,  94],\n",
              "         ...,\n",
              "         [ 78,  85,  83],\n",
              "         [ 79,  85,  83],\n",
              "         [ 80,  86,  84]]],\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "       [[[ 35, 178, 235],\n",
              "         [ 40, 176, 239],\n",
              "         [ 42, 176, 241],\n",
              "         ...,\n",
              "         [ 99, 177, 219],\n",
              "         [ 79, 147, 197],\n",
              "         [ 89, 148, 189]],\n",
              "\n",
              "        [[ 57, 182, 234],\n",
              "         [ 44, 184, 250],\n",
              "         [ 50, 183, 240],\n",
              "         ...,\n",
              "         [156, 182, 200],\n",
              "         [141, 177, 206],\n",
              "         [116, 149, 175]],\n",
              "\n",
              "        [[ 98, 197, 237],\n",
              "         [ 64, 189, 252],\n",
              "         [ 69, 192, 245],\n",
              "         ...,\n",
              "         [188, 195, 206],\n",
              "         [119, 135, 147],\n",
              "         [ 61,  79,  90]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 73,  79,  77],\n",
              "         [ 53,  63,  68],\n",
              "         [ 54,  68,  80],\n",
              "         ...,\n",
              "         [ 17,  40,  64],\n",
              "         [ 21,  36,  51],\n",
              "         [ 33,  48,  49]],\n",
              "\n",
              "        [[ 61,  68,  75],\n",
              "         [ 55,  70,  86],\n",
              "         [ 57,  79, 103],\n",
              "         ...,\n",
              "         [ 24,  48,  72],\n",
              "         [ 17,  35,  53],\n",
              "         [  7,  23,  32]],\n",
              "\n",
              "        [[ 44,  56,  73],\n",
              "         [ 46,  66,  88],\n",
              "         [ 49,  77, 105],\n",
              "         ...,\n",
              "         [ 27,  52,  77],\n",
              "         [ 21,  43,  66],\n",
              "         [ 12,  31,  50]]],\n",
              "\n",
              "\n",
              "       [[[189, 211, 240],\n",
              "         [186, 208, 236],\n",
              "         [185, 207, 235],\n",
              "         ...,\n",
              "         [175, 195, 224],\n",
              "         [172, 194, 222],\n",
              "         [169, 194, 220]],\n",
              "\n",
              "        [[194, 210, 239],\n",
              "         [191, 207, 236],\n",
              "         [190, 206, 235],\n",
              "         ...,\n",
              "         [173, 192, 220],\n",
              "         [171, 191, 218],\n",
              "         [167, 190, 216]],\n",
              "\n",
              "        [[208, 219, 244],\n",
              "         [205, 216, 240],\n",
              "         [204, 215, 239],\n",
              "         ...,\n",
              "         [175, 191, 217],\n",
              "         [172, 190, 216],\n",
              "         [169, 191, 215]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[207, 199, 181],\n",
              "         [203, 195, 175],\n",
              "         [203, 196, 173],\n",
              "         ...,\n",
              "         [135, 132, 127],\n",
              "         [162, 158, 150],\n",
              "         [168, 163, 151]],\n",
              "\n",
              "        [[198, 190, 170],\n",
              "         [189, 181, 159],\n",
              "         [180, 172, 147],\n",
              "         ...,\n",
              "         [178, 171, 160],\n",
              "         [175, 169, 156],\n",
              "         [175, 169, 154]],\n",
              "\n",
              "        [[198, 189, 173],\n",
              "         [189, 181, 162],\n",
              "         [178, 170, 149],\n",
              "         ...,\n",
              "         [195, 184, 169],\n",
              "         [196, 189, 171],\n",
              "         [195, 190, 171]]],\n",
              "\n",
              "\n",
              "       [[[229, 229, 239],\n",
              "         [236, 237, 247],\n",
              "         [234, 236, 247],\n",
              "         ...,\n",
              "         [217, 219, 233],\n",
              "         [221, 223, 234],\n",
              "         [222, 223, 233]],\n",
              "\n",
              "        [[222, 221, 229],\n",
              "         [239, 239, 249],\n",
              "         [233, 234, 246],\n",
              "         ...,\n",
              "         [223, 223, 236],\n",
              "         [227, 228, 238],\n",
              "         [210, 211, 220]],\n",
              "\n",
              "        [[213, 206, 211],\n",
              "         [234, 232, 239],\n",
              "         [231, 233, 244],\n",
              "         ...,\n",
              "         [220, 220, 232],\n",
              "         [220, 219, 232],\n",
              "         [202, 203, 215]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[150, 143, 135],\n",
              "         [140, 135, 127],\n",
              "         [132, 127, 120],\n",
              "         ...,\n",
              "         [224, 222, 218],\n",
              "         [230, 228, 225],\n",
              "         [241, 241, 238]],\n",
              "\n",
              "        [[137, 132, 126],\n",
              "         [130, 127, 120],\n",
              "         [125, 121, 115],\n",
              "         ...,\n",
              "         [181, 180, 178],\n",
              "         [202, 201, 198],\n",
              "         [212, 211, 207]],\n",
              "\n",
              "        [[122, 119, 114],\n",
              "         [118, 116, 110],\n",
              "         [120, 116, 111],\n",
              "         ...,\n",
              "         [179, 177, 173],\n",
              "         [164, 164, 162],\n",
              "         [163, 163, 161]]]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXS1s6spgSxQ",
        "outputId": "f1187316-4bdf-4463-a7a2-dd68223a5c60"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz02CR4GhFrH",
        "outputId": "ab68f6b6-92fd-463d-b172-93f12d1f84eb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSJ-nCATin8j",
        "outputId": "804abd3a-357f-45bd-e736-4dde143fd59b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 59,  62,  63],\n",
              "        [ 43,  46,  45],\n",
              "        [ 50,  48,  43],\n",
              "        ...,\n",
              "        [158, 132, 108],\n",
              "        [152, 125, 102],\n",
              "        [148, 124, 103]],\n",
              "\n",
              "       [[ 16,  20,  20],\n",
              "        [  0,   0,   0],\n",
              "        [ 18,   8,   0],\n",
              "        ...,\n",
              "        [123,  88,  55],\n",
              "        [119,  83,  50],\n",
              "        [122,  87,  57]],\n",
              "\n",
              "       [[ 25,  24,  21],\n",
              "        [ 16,   7,   0],\n",
              "        [ 49,  27,   8],\n",
              "        ...,\n",
              "        [118,  84,  50],\n",
              "        [120,  84,  50],\n",
              "        [109,  73,  42]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[208, 170,  96],\n",
              "        [201, 153,  34],\n",
              "        [198, 161,  26],\n",
              "        ...,\n",
              "        [160, 133,  70],\n",
              "        [ 56,  31,   7],\n",
              "        [ 53,  34,  20]],\n",
              "\n",
              "       [[180, 139,  96],\n",
              "        [173, 123,  42],\n",
              "        [186, 144,  30],\n",
              "        ...,\n",
              "        [184, 148,  94],\n",
              "        [ 97,  62,  34],\n",
              "        [ 83,  53,  34]],\n",
              "\n",
              "       [[177, 144, 116],\n",
              "        [168, 129,  94],\n",
              "        [179, 142,  87],\n",
              "        ...,\n",
              "        [216, 184, 140],\n",
              "        [151, 118,  84],\n",
              "        [123,  92,  72]]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "1zVj52zGitmq",
        "outputId": "208f261f-9ffa-4bb4-d6fc-b3b7f027b292"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa7550220d0>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X_train[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "tamKvX4Ci2qC",
        "outputId": "529cf205-6320-4f1a-ea06-77fbf175b2be"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fa754f72ca0>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes=['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
      ],
      "metadata": {
        "id": "h57U54KLbiHQ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wqV1nX-boxS",
        "outputId": "a417efa2-eea4-4b54-bf63-4d17493d21f2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnVuCEyEbwBp",
        "outputId": "7d18691c-0cf3-42a6-f7a4-cbdfd9ef0f69"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6],\n",
              "       [9],\n",
              "       [9],\n",
              "       [4],\n",
              "       [1]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train=y_train.reshape(-1,)\n",
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gU5HYSeikDIg",
        "outputId": "6945e121-0dc6-47cf-f908-22a360211760"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6, 9, 9, ..., 9, 1, 1], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes[9]"
      ],
      "metadata": {
        "id": "KBlA9N4OkN8q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4b0f643c-4d56-46d0-ce10-371fbf339f37"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'truck'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_samples(X,y,index):\n",
        "  plt.figure(figsize=(15,2))\n",
        "  plt.imshow(X[index])\n",
        "  plt.xlabel(classes[y[index]])\n"
      ],
      "metadata": {
        "id": "nmIpHKGvktAf"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for example \n",
        "plot_samples(X_train,y_train,1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "T8mudDtmlIX4",
        "outputId": "66b9fb4f-b8ef-42d7-829e-7f0ed02262d5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x144 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the data between 0 to 1 by dividing it by 255. as its an unsigned data between 0-255 range\n",
        "X_train=X_train/255\n",
        "X_test=X_test/255"
      ],
      "metadata": {
        "id": "fqRmUiq8lSWG"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout,BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "p_nvlg7rmUhO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential([layers.Flatten(input_shape=(32,32,3)),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dense(10,activation='softmax')])\n",
        "model.compile(optimizer='adam',metrics=['accuracy'],loss=['sparse_categorical_crossentropy'])"
      ],
      "metadata": {
        "id": "F0T0L8GGnEBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_full,y_train_full,epochs=200)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3mRioxCncML",
        "outputId": "24fc58a9-787a-4d32-c1e6-d2bc5f8c29fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1563/1563 [==============================] - 9s 5ms/step - loss: 2.0664 - accuracy: 0.2361\n",
            "Epoch 2/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8826 - accuracy: 0.3084\n",
            "Epoch 3/200\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.8326 - accuracy: 0.3334\n",
            "Epoch 4/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7851 - accuracy: 0.3534\n",
            "Epoch 5/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.7535 - accuracy: 0.3677\n",
            "Epoch 6/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.7303 - accuracy: 0.3778\n",
            "Epoch 7/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7020 - accuracy: 0.3909\n",
            "Epoch 8/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.9516 - accuracy: 0.2740\n",
            "Epoch 9/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.9593 - accuracy: 0.2521\n",
            "Epoch 10/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8122 - accuracy: 0.3264\n",
            "Epoch 11/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3080 - accuracy: 0.1605\n",
            "Epoch 12/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3174 - accuracy: 0.1034\n",
            "Epoch 13/200\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.3132 - accuracy: 0.0989\n",
            "Epoch 14/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3133 - accuracy: 0.0974\n",
            "Epoch 15/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3146 - accuracy: 0.1006\n",
            "Epoch 16/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3149 - accuracy: 0.1005\n",
            "Epoch 17/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3150 - accuracy: 0.1017\n",
            "Epoch 18/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3164 - accuracy: 0.0989\n",
            "Epoch 19/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3164 - accuracy: 0.0998\n",
            "Epoch 20/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3159 - accuracy: 0.0992\n",
            "Epoch 21/200\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.4802 - accuracy: 0.0963\n",
            "Epoch 22/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3152 - accuracy: 0.0999\n",
            "Epoch 23/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3148 - accuracy: 0.0999\n",
            "Epoch 24/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3160 - accuracy: 0.0999\n",
            "Epoch 25/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3150 - accuracy: 0.0980\n",
            "Epoch 26/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3149 - accuracy: 0.0999\n",
            "Epoch 27/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3158 - accuracy: 0.1004\n",
            "Epoch 28/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3150 - accuracy: 0.0990\n",
            "Epoch 29/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3158 - accuracy: 0.0990\n",
            "Epoch 30/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3150 - accuracy: 0.1010\n",
            "Epoch 31/200\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 2.3154 - accuracy: 0.0997\n",
            "Epoch 32/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3163 - accuracy: 0.1001\n",
            "Epoch 33/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3149 - accuracy: 0.0988\n",
            "Epoch 34/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3148 - accuracy: 0.0984\n",
            "Epoch 35/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3160 - accuracy: 0.0984\n",
            "Epoch 36/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3156 - accuracy: 0.1023\n",
            "Epoch 37/200\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.3151 - accuracy: 0.1025\n",
            "Epoch 38/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3163 - accuracy: 0.0997\n",
            "Epoch 39/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3147 - accuracy: 0.1006\n",
            "Epoch 40/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3158 - accuracy: 0.1011\n",
            "Epoch 41/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3153 - accuracy: 0.1008\n",
            "Epoch 42/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3155 - accuracy: 0.1013\n",
            "Epoch 43/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3158 - accuracy: 0.1006\n",
            "Epoch 44/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3158 - accuracy: 0.0983\n",
            "Epoch 45/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3149 - accuracy: 0.0994\n",
            "Epoch 46/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3151 - accuracy: 0.1015\n",
            "Epoch 47/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3160 - accuracy: 0.0969\n",
            "Epoch 48/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3160 - accuracy: 0.1006\n",
            "Epoch 49/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3154 - accuracy: 0.0990\n",
            "Epoch 50/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3158 - accuracy: 0.1009\n",
            "Epoch 51/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3154 - accuracy: 0.0990\n",
            "Epoch 52/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3158 - accuracy: 0.1016\n",
            "Epoch 53/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3149 - accuracy: 0.1007\n",
            "Epoch 54/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3164 - accuracy: 0.0989\n",
            "Epoch 55/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3161 - accuracy: 0.0985\n",
            "Epoch 56/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3156 - accuracy: 0.0994\n",
            "Epoch 57/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3160 - accuracy: 0.1025\n",
            "Epoch 58/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3162 - accuracy: 0.0979\n",
            "Epoch 59/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3157 - accuracy: 0.0985\n",
            "Epoch 60/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3156 - accuracy: 0.1000\n",
            "Epoch 61/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3158 - accuracy: 0.1024\n",
            "Epoch 62/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3141 - accuracy: 0.1019\n",
            "Epoch 63/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3156 - accuracy: 0.0997\n",
            "Epoch 64/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3152 - accuracy: 0.1018\n",
            "Epoch 65/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3161 - accuracy: 0.0995\n",
            "Epoch 66/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3145 - accuracy: 0.1006\n",
            "Epoch 67/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3166 - accuracy: 0.1006\n",
            "Epoch 68/200\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.3156 - accuracy: 0.1001\n",
            "Epoch 69/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3156 - accuracy: 0.0992\n",
            "Epoch 70/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3159 - accuracy: 0.0993\n",
            "Epoch 71/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3158 - accuracy: 0.1008\n",
            "Epoch 72/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3164 - accuracy: 0.0974\n",
            "Epoch 73/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3149 - accuracy: 0.0974\n",
            "Epoch 74/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3163 - accuracy: 0.1004\n",
            "Epoch 75/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3156 - accuracy: 0.0985\n",
            "Epoch 76/200\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.3160 - accuracy: 0.1006\n",
            "Epoch 77/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3163 - accuracy: 0.0993\n",
            "Epoch 78/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3156 - accuracy: 0.1001\n",
            "Epoch 79/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3159 - accuracy: 0.1011\n",
            "Epoch 80/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3161 - accuracy: 0.0989\n",
            "Epoch 81/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 2.3153 - accuracy: 0.1002\n",
            "Epoch 82/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3153 - accuracy: 0.1024\n",
            "Epoch 83/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3149 - accuracy: 0.0997\n",
            "Epoch 84/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3155 - accuracy: 0.0990\n",
            "Epoch 85/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3158 - accuracy: 0.1001\n",
            "Epoch 86/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3149 - accuracy: 0.1003\n",
            "Epoch 87/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3153 - accuracy: 0.0999\n",
            "Epoch 88/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3158 - accuracy: 0.0997\n",
            "Epoch 89/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3159 - accuracy: 0.1021\n",
            "Epoch 90/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3159 - accuracy: 0.0997\n",
            "Epoch 91/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3156 - accuracy: 0.1005\n",
            "Epoch 92/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3167 - accuracy: 0.0993\n",
            "Epoch 93/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3151 - accuracy: 0.1010\n",
            "Epoch 94/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3147 - accuracy: 0.0968\n",
            "Epoch 95/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3145 - accuracy: 0.1027\n",
            "Epoch 96/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3162 - accuracy: 0.1006\n",
            "Epoch 97/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3146 - accuracy: 0.1004\n",
            "Epoch 98/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3151 - accuracy: 0.1000\n",
            "Epoch 99/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3163 - accuracy: 0.0982\n",
            "Epoch 100/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3154 - accuracy: 0.0985\n",
            "Epoch 101/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3154 - accuracy: 0.1015\n",
            "Epoch 102/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3157 - accuracy: 0.0983\n",
            "Epoch 103/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3157 - accuracy: 0.0995\n",
            "Epoch 104/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3149 - accuracy: 0.0984\n",
            "Epoch 105/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3165 - accuracy: 0.0983\n",
            "Epoch 106/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3160 - accuracy: 0.0996\n",
            "Epoch 107/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3159 - accuracy: 0.1015\n",
            "Epoch 108/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3155 - accuracy: 0.1018\n",
            "Epoch 109/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3150 - accuracy: 0.1020\n",
            "Epoch 110/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3162 - accuracy: 0.0981\n",
            "Epoch 111/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3156 - accuracy: 0.1018\n",
            "Epoch 112/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3158 - accuracy: 0.1015\n",
            "Epoch 113/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3156 - accuracy: 0.0983\n",
            "Epoch 114/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3153 - accuracy: 0.0998\n",
            "Epoch 115/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3165 - accuracy: 0.0989\n",
            "Epoch 116/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3160 - accuracy: 0.0989\n",
            "Epoch 117/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3164 - accuracy: 0.0985\n",
            "Epoch 118/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3160 - accuracy: 0.0989\n",
            "Epoch 119/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3148 - accuracy: 0.1005\n",
            "Epoch 120/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3150 - accuracy: 0.0991\n",
            "Epoch 121/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3150 - accuracy: 0.1012\n",
            "Epoch 122/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3156 - accuracy: 0.1005\n",
            "Epoch 123/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3151 - accuracy: 0.1001\n",
            "Epoch 124/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3155 - accuracy: 0.1007\n",
            "Epoch 125/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3169 - accuracy: 0.0998\n",
            "Epoch 126/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3161 - accuracy: 0.1008\n",
            "Epoch 127/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3160 - accuracy: 0.0973\n",
            "Epoch 128/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3157 - accuracy: 0.1009\n",
            "Epoch 129/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 2.3166 - accuracy: 0.0993\n",
            "Epoch 130/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3152 - accuracy: 0.1015\n",
            "Epoch 131/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3161 - accuracy: 0.0999\n",
            "Epoch 132/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3158 - accuracy: 0.1016\n",
            "Epoch 133/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3145 - accuracy: 0.0994\n",
            "Epoch 134/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3165 - accuracy: 0.0983\n",
            "Epoch 135/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3151 - accuracy: 0.1029\n",
            "Epoch 136/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3164 - accuracy: 0.0995\n",
            "Epoch 137/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3155 - accuracy: 0.0994\n",
            "Epoch 138/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3152 - accuracy: 0.0990\n",
            "Epoch 139/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3154 - accuracy: 0.1010\n",
            "Epoch 140/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3163 - accuracy: 0.0993\n",
            "Epoch 141/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3156 - accuracy: 0.1004\n",
            "Epoch 142/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3142 - accuracy: 0.1012\n",
            "Epoch 143/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3159 - accuracy: 0.1005\n",
            "Epoch 144/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3151 - accuracy: 0.1010\n",
            "Epoch 145/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3156 - accuracy: 0.1004\n",
            "Epoch 146/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3161 - accuracy: 0.0981\n",
            "Epoch 147/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3163 - accuracy: 0.0979\n",
            "Epoch 148/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3155 - accuracy: 0.0969\n",
            "Epoch 149/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3153 - accuracy: 0.1028\n",
            "Epoch 150/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3152 - accuracy: 0.0985\n",
            "Epoch 151/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3149 - accuracy: 0.1027\n",
            "Epoch 152/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3163 - accuracy: 0.0980\n",
            "Epoch 153/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3170 - accuracy: 0.0976\n",
            "Epoch 154/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3154 - accuracy: 0.1001\n",
            "Epoch 155/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3156 - accuracy: 0.0994\n",
            "Epoch 156/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3156 - accuracy: 0.1016\n",
            "Epoch 157/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3146 - accuracy: 0.0994\n",
            "Epoch 158/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3163 - accuracy: 0.0972\n",
            "Epoch 159/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3163 - accuracy: 0.0983\n",
            "Epoch 160/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3152 - accuracy: 0.1005\n",
            "Epoch 161/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3155 - accuracy: 0.1002\n",
            "Epoch 162/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3165 - accuracy: 0.0981\n",
            "Epoch 163/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3148 - accuracy: 0.1017\n",
            "Epoch 164/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3155 - accuracy: 0.0999\n",
            "Epoch 165/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3146 - accuracy: 0.1026\n",
            "Epoch 166/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3157 - accuracy: 0.1003\n",
            "Epoch 167/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3153 - accuracy: 0.1014\n",
            "Epoch 168/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3153 - accuracy: 0.0997\n",
            "Epoch 169/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3160 - accuracy: 0.0982\n",
            "Epoch 170/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3155 - accuracy: 0.1000\n",
            "Epoch 171/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3151 - accuracy: 0.0991\n",
            "Epoch 172/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3155 - accuracy: 0.1020\n",
            "Epoch 173/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3160 - accuracy: 0.1001\n",
            "Epoch 174/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3156 - accuracy: 0.0978\n",
            "Epoch 175/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3157 - accuracy: 0.0991\n",
            "Epoch 176/200\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 2.3158 - accuracy: 0.0992\n",
            "Epoch 177/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3150 - accuracy: 0.1014\n",
            "Epoch 178/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3145 - accuracy: 0.1025\n",
            "Epoch 179/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3151 - accuracy: 0.0984\n",
            "Epoch 180/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3151 - accuracy: 0.0991\n",
            "Epoch 181/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3159 - accuracy: 0.1001\n",
            "Epoch 182/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3153 - accuracy: 0.0997\n",
            "Epoch 183/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3159 - accuracy: 0.0980\n",
            "Epoch 184/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3156 - accuracy: 0.1004\n",
            "Epoch 185/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3157 - accuracy: 0.1000\n",
            "Epoch 186/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3161 - accuracy: 0.1022\n",
            "Epoch 187/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3165 - accuracy: 0.0990\n",
            "Epoch 188/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3160 - accuracy: 0.0978\n",
            "Epoch 189/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3150 - accuracy: 0.1006\n",
            "Epoch 190/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3152 - accuracy: 0.1022\n",
            "Epoch 191/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3158 - accuracy: 0.1001\n",
            "Epoch 192/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3158 - accuracy: 0.1012\n",
            "Epoch 193/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3142 - accuracy: 0.1014\n",
            "Epoch 194/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3157 - accuracy: 0.1016\n",
            "Epoch 195/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3159 - accuracy: 0.1010\n",
            "Epoch 196/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3150 - accuracy: 0.0996\n",
            "Epoch 197/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3151 - accuracy: 0.0990\n",
            "Epoch 198/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3159 - accuracy: 0.1001\n",
            "Epoch 199/200\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.3158 - accuracy: 0.1003\n",
            "Epoch 200/200\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3142 - accuracy: 0.1038\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f82e417c820>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential([layers.Flatten(input_shape=(32,32,3)),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dropout(0.2),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dropout(0.2),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dropout(0.2),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dropout(0.2),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dropout(0.2),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dropout(0.2),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dropout(0.2),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dropout(0.2),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dropout(0.2),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dropout(0.2),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dropout(0.2),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dropout(0.2),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dropout(0.2),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dropout(0.2),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dropout(0.2),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dropout(0.2),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dropout(0.2),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dropout(0.2),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dropout(0.2),\n",
        "                  layers.Dense(100,activation='elu',kernel_initializer='he_uniform'),\n",
        "                  layers.Dropout(0.2),\n",
        "                  layers.Dense(10,activation='softmax')])\n",
        "model.compile(optimizer='adam',metrics=['accuracy'],loss=['sparse_categorical_crossentropy'])"
      ],
      "metadata": {
        "id": "tyoXjb49p5lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jZRyzvqup7y",
        "outputId": "ad346ca5-ddb1-4e6f-af27-cd28d833236c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_4 (Flatten)         (None, 3072)              0         \n",
            "                                                                 \n",
            " dense_77 (Dense)            (None, 100)               307300    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100)               0         \n",
            "                                                                 \n",
            " dense_78 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_79 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_80 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_81 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_82 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_83 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_84 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_85 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_86 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_87 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 100)               0         \n",
            "                                                                 \n",
            " dense_88 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 100)               0         \n",
            "                                                                 \n",
            " dense_89 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 100)               0         \n",
            "                                                                 \n",
            " dense_90 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 100)               0         \n",
            "                                                                 \n",
            " dense_91 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 100)               0         \n",
            "                                                                 \n",
            " dense_92 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 100)               0         \n",
            "                                                                 \n",
            " dense_93 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_16 (Dropout)        (None, 100)               0         \n",
            "                                                                 \n",
            " dense_94 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 100)               0         \n",
            "                                                                 \n",
            " dense_95 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 100)               0         \n",
            "                                                                 \n",
            " dense_96 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 100)               0         \n",
            "                                                                 \n",
            " dense_97 (Dense)            (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 500,210\n",
            "Trainable params: 500,210\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_full,y_train_full,validation_split=0.2,epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pin5assDwNBE",
        "outputId": "3781457b-68aa-4a67-ecda-1c8a173608fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1250/1250 [==============================] - 12s 8ms/step - loss: 2.8016 - accuracy: 0.1025 - val_loss: 2.3269 - val_accuracy: 0.0980\n",
            "Epoch 2/100\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 2.3577 - accuracy: 0.1018 - val_loss: 2.3159 - val_accuracy: 0.0997\n",
            "Epoch 3/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 2.2283 - accuracy: 0.1426 - val_loss: 2.1333 - val_accuracy: 0.1614\n",
            "Epoch 4/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.1408 - accuracy: 0.1654 - val_loss: 2.2801 - val_accuracy: 0.1209\n",
            "Epoch 5/100\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 2.1167 - accuracy: 0.1684 - val_loss: 2.0800 - val_accuracy: 0.1928\n",
            "Epoch 6/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0947 - accuracy: 0.1726 - val_loss: 2.1135 - val_accuracy: 0.1746\n",
            "Epoch 7/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0754 - accuracy: 0.1763 - val_loss: 2.1025 - val_accuracy: 0.1820\n",
            "Epoch 8/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0750 - accuracy: 0.1796 - val_loss: 2.0652 - val_accuracy: 0.1706\n",
            "Epoch 9/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 2.0633 - accuracy: 0.1780 - val_loss: 2.1194 - val_accuracy: 0.1581\n",
            "Epoch 10/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0585 - accuracy: 0.1802 - val_loss: 2.0800 - val_accuracy: 0.1919\n",
            "Epoch 11/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0564 - accuracy: 0.1819 - val_loss: 2.0166 - val_accuracy: 0.1977\n",
            "Epoch 12/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 2.0662 - accuracy: 0.1805 - val_loss: 2.0668 - val_accuracy: 0.1738\n",
            "Epoch 13/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0942 - accuracy: 0.1753 - val_loss: 2.0642 - val_accuracy: 0.1766\n",
            "Epoch 14/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0604 - accuracy: 0.1858 - val_loss: 2.0301 - val_accuracy: 0.2120\n",
            "Epoch 15/100\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 2.0569 - accuracy: 0.1899 - val_loss: 2.0749 - val_accuracy: 0.1927\n",
            "Epoch 16/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0599 - accuracy: 0.1902 - val_loss: 2.0815 - val_accuracy: 0.1895\n",
            "Epoch 17/100\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 2.1245 - accuracy: 0.1697 - val_loss: 2.1127 - val_accuracy: 0.1667\n",
            "Epoch 18/100\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 2.0647 - accuracy: 0.1814 - val_loss: 2.1371 - val_accuracy: 0.1413\n",
            "Epoch 19/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0576 - accuracy: 0.1872 - val_loss: 2.0691 - val_accuracy: 0.1744\n",
            "Epoch 20/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0941 - accuracy: 0.1761 - val_loss: 2.1359 - val_accuracy: 0.1770\n",
            "Epoch 21/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0626 - accuracy: 0.1811 - val_loss: 2.1678 - val_accuracy: 0.1504\n",
            "Epoch 22/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.1298 - accuracy: 0.1813 - val_loss: 2.2304 - val_accuracy: 0.1467\n",
            "Epoch 23/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 2.0676 - accuracy: 0.1855 - val_loss: 2.0734 - val_accuracy: 0.1661\n",
            "Epoch 24/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0456 - accuracy: 0.1903 - val_loss: 2.0966 - val_accuracy: 0.1719\n",
            "Epoch 25/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0516 - accuracy: 0.1957 - val_loss: 2.1673 - val_accuracy: 0.1411\n",
            "Epoch 26/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 2.0639 - accuracy: 0.1896 - val_loss: 2.0778 - val_accuracy: 0.1782\n",
            "Epoch 27/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0561 - accuracy: 0.1926 - val_loss: 2.0554 - val_accuracy: 0.1931\n",
            "Epoch 28/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 2.0404 - accuracy: 0.1925 - val_loss: 2.0814 - val_accuracy: 0.1595\n",
            "Epoch 29/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 2.0500 - accuracy: 0.1879 - val_loss: 2.1295 - val_accuracy: 0.1575\n",
            "Epoch 30/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0323 - accuracy: 0.1905 - val_loss: 2.1006 - val_accuracy: 0.1684\n",
            "Epoch 31/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0277 - accuracy: 0.1947 - val_loss: 2.0292 - val_accuracy: 0.1927\n",
            "Epoch 32/100\n",
            "1250/1250 [==============================] - 10s 8ms/step - loss: 2.0178 - accuracy: 0.2019 - val_loss: 2.0743 - val_accuracy: 0.1949\n",
            "Epoch 33/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0207 - accuracy: 0.2033 - val_loss: 2.1325 - val_accuracy: 0.1521\n",
            "Epoch 34/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0207 - accuracy: 0.2058 - val_loss: 2.0498 - val_accuracy: 0.1923\n",
            "Epoch 35/100\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 2.0202 - accuracy: 0.2091 - val_loss: 2.2140 - val_accuracy: 0.1375\n",
            "Epoch 36/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0369 - accuracy: 0.2016 - val_loss: 2.0937 - val_accuracy: 0.1870\n",
            "Epoch 37/100\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 2.0252 - accuracy: 0.2016 - val_loss: 2.3072 - val_accuracy: 0.1390\n",
            "Epoch 38/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0268 - accuracy: 0.2040 - val_loss: 2.1322 - val_accuracy: 0.1650\n",
            "Epoch 39/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 13.7892 - accuracy: 0.1753 - val_loss: 2.1624 - val_accuracy: 0.1585\n",
            "Epoch 40/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.1009 - accuracy: 0.1797 - val_loss: 2.1064 - val_accuracy: 0.1765\n",
            "Epoch 41/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0473 - accuracy: 0.1901 - val_loss: 2.0858 - val_accuracy: 0.1808\n",
            "Epoch 42/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0393 - accuracy: 0.1902 - val_loss: 2.0670 - val_accuracy: 0.1986\n",
            "Epoch 43/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0446 - accuracy: 0.1923 - val_loss: 2.0759 - val_accuracy: 0.1776\n",
            "Epoch 44/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0299 - accuracy: 0.1957 - val_loss: 2.0520 - val_accuracy: 0.1937\n",
            "Epoch 45/100\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 2.0249 - accuracy: 0.2011 - val_loss: 2.0734 - val_accuracy: 0.1814\n",
            "Epoch 46/100\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 2.0250 - accuracy: 0.2013 - val_loss: 2.0728 - val_accuracy: 0.1708\n",
            "Epoch 47/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0221 - accuracy: 0.2028 - val_loss: 2.1642 - val_accuracy: 0.1565\n",
            "Epoch 48/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0546 - accuracy: 0.2021 - val_loss: 2.3756 - val_accuracy: 0.0980\n",
            "Epoch 49/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0871 - accuracy: 0.1795 - val_loss: 2.1416 - val_accuracy: 0.1587\n",
            "Epoch 50/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0701 - accuracy: 0.1840 - val_loss: 2.1182 - val_accuracy: 0.1683\n",
            "Epoch 51/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0837 - accuracy: 0.1834 - val_loss: 2.1346 - val_accuracy: 0.1667\n",
            "Epoch 52/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0611 - accuracy: 0.1850 - val_loss: 2.0490 - val_accuracy: 0.1800\n",
            "Epoch 53/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0354 - accuracy: 0.1900 - val_loss: 2.0893 - val_accuracy: 0.1789\n",
            "Epoch 54/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0751 - accuracy: 0.1878 - val_loss: 2.0931 - val_accuracy: 0.1829\n",
            "Epoch 55/100\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 2.1401 - accuracy: 0.1834 - val_loss: 2.0951 - val_accuracy: 0.1906\n",
            "Epoch 56/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 2.1033 - accuracy: 0.1819 - val_loss: 2.0791 - val_accuracy: 0.1869\n",
            "Epoch 57/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0526 - accuracy: 0.1934 - val_loss: 2.0740 - val_accuracy: 0.1895\n",
            "Epoch 58/100\n",
            "1250/1250 [==============================] - 7s 6ms/step - loss: 2.0409 - accuracy: 0.1980 - val_loss: 2.0255 - val_accuracy: 0.1928\n",
            "Epoch 59/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.1282 - accuracy: 0.2064 - val_loss: 2.1100 - val_accuracy: 0.1938\n",
            "Epoch 60/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0837 - accuracy: 0.1966 - val_loss: 2.0788 - val_accuracy: 0.1980\n",
            "Epoch 61/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0851 - accuracy: 0.1906 - val_loss: 2.1180 - val_accuracy: 0.1729\n",
            "Epoch 62/100\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 2.2270 - accuracy: 0.1810 - val_loss: 2.2793 - val_accuracy: 0.1058\n",
            "Epoch 63/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0581 - accuracy: 0.1888 - val_loss: 2.0534 - val_accuracy: 0.1850\n",
            "Epoch 64/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0540 - accuracy: 0.1930 - val_loss: 2.1519 - val_accuracy: 0.1557\n",
            "Epoch 65/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0459 - accuracy: 0.1921 - val_loss: 2.0833 - val_accuracy: 0.1941\n",
            "Epoch 66/100\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 2.0592 - accuracy: 0.1941 - val_loss: 2.0964 - val_accuracy: 0.1842\n",
            "Epoch 67/100\n",
            "1250/1250 [==============================] - 9s 8ms/step - loss: 2.0682 - accuracy: 0.2009 - val_loss: 2.0722 - val_accuracy: 0.1847\n",
            "Epoch 68/100\n",
            "1250/1250 [==============================] - 11s 9ms/step - loss: 2.0441 - accuracy: 0.2007 - val_loss: 2.1806 - val_accuracy: 0.1466\n",
            "Epoch 69/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0886 - accuracy: 0.1959 - val_loss: 2.0731 - val_accuracy: 0.1745\n",
            "Epoch 70/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0562 - accuracy: 0.1900 - val_loss: 2.0435 - val_accuracy: 0.1848\n",
            "Epoch 71/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0474 - accuracy: 0.1995 - val_loss: 2.0839 - val_accuracy: 0.1619\n",
            "Epoch 72/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0969 - accuracy: 0.1874 - val_loss: 2.0938 - val_accuracy: 0.1771\n",
            "Epoch 73/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0505 - accuracy: 0.1948 - val_loss: 2.0712 - val_accuracy: 0.1805\n",
            "Epoch 74/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0675 - accuracy: 0.1969 - val_loss: 2.0241 - val_accuracy: 0.1944\n",
            "Epoch 75/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0534 - accuracy: 0.2027 - val_loss: 2.1042 - val_accuracy: 0.1800\n",
            "Epoch 76/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0617 - accuracy: 0.1988 - val_loss: 2.1020 - val_accuracy: 0.2056\n",
            "Epoch 77/100\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 2.6255 - accuracy: 0.1679 - val_loss: 2.1284 - val_accuracy: 0.1820\n",
            "Epoch 78/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.1335 - accuracy: 0.1798 - val_loss: 2.1084 - val_accuracy: 0.1837\n",
            "Epoch 79/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0841 - accuracy: 0.1873 - val_loss: 2.0968 - val_accuracy: 0.1857\n",
            "Epoch 80/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0707 - accuracy: 0.1913 - val_loss: 2.0825 - val_accuracy: 0.1757\n",
            "Epoch 81/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0574 - accuracy: 0.1897 - val_loss: 2.1153 - val_accuracy: 0.1666\n",
            "Epoch 82/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0380 - accuracy: 0.1945 - val_loss: 2.0264 - val_accuracy: 0.1893\n",
            "Epoch 83/100\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 2.4395 - accuracy: 0.1984 - val_loss: 2.2039 - val_accuracy: 0.1655\n",
            "Epoch 84/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0647 - accuracy: 0.1971 - val_loss: 2.1202 - val_accuracy: 0.1720\n",
            "Epoch 85/100\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 2.0612 - accuracy: 0.1900 - val_loss: 2.0989 - val_accuracy: 0.2037\n",
            "Epoch 86/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0696 - accuracy: 0.2006 - val_loss: 2.0558 - val_accuracy: 0.2100\n",
            "Epoch 87/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0556 - accuracy: 0.2016 - val_loss: 2.0694 - val_accuracy: 0.1986\n",
            "Epoch 88/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0220 - accuracy: 0.2094 - val_loss: 2.0407 - val_accuracy: 0.2068\n",
            "Epoch 89/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0433 - accuracy: 0.2009 - val_loss: 2.0798 - val_accuracy: 0.1815\n",
            "Epoch 90/100\n",
            "1250/1250 [==============================] - 8s 6ms/step - loss: 2.0295 - accuracy: 0.2008 - val_loss: 2.1467 - val_accuracy: 0.1644\n",
            "Epoch 91/100\n",
            "1250/1250 [==============================] - 10s 8ms/step - loss: 2.0146 - accuracy: 0.2081 - val_loss: 2.0894 - val_accuracy: 0.1806\n",
            "Epoch 92/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0004 - accuracy: 0.2130 - val_loss: 2.0386 - val_accuracy: 0.2096\n",
            "Epoch 93/100\n",
            "1250/1250 [==============================] - 9s 8ms/step - loss: 1.9918 - accuracy: 0.2134 - val_loss: 2.0927 - val_accuracy: 0.1842\n",
            "Epoch 94/100\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 1.9993 - accuracy: 0.2156 - val_loss: 2.1105 - val_accuracy: 0.1920\n",
            "Epoch 95/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0081 - accuracy: 0.2101 - val_loss: 2.0226 - val_accuracy: 0.1962\n",
            "Epoch 96/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.0460 - accuracy: 0.2063 - val_loss: 2.2371 - val_accuracy: 0.1373\n",
            "Epoch 97/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 2.3556 - accuracy: 0.1748 - val_loss: 2.1520 - val_accuracy: 0.1669\n",
            "Epoch 98/100\n",
            "1250/1250 [==============================] - 8s 7ms/step - loss: 2.0614 - accuracy: 0.1884 - val_loss: 2.1840 - val_accuracy: 0.1613\n",
            "Epoch 99/100\n",
            "1250/1250 [==============================] - 9s 7ms/step - loss: 5.4749 - accuracy: 0.1837 - val_loss: 2.3390 - val_accuracy: 0.1183\n",
            "Epoch 100/100\n",
            "1250/1250 [==============================] - 10s 8ms/step - loss: 2.1146 - accuracy: 0.1732 - val_loss: 2.1217 - val_accuracy: 0.1771\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f825575ba00>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "with batch normalization "
      ],
      "metadata": {
        "id": "f92kh0fi9vnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model3=Sequential([layers.Flatten(input_shape=(32,32,3)),\n",
        "                  layers.Dense(100,activation='selu',kernel_initializer='he_uniform'),\n",
        "                  layers.BatchNormalization(),      \n",
        "                  layers.Dense(100,activation='selu',kernel_initializer='he_uniform'),\n",
        "                  layers.BatchNormalization(), \n",
        "                  layers.Dense(100,activation='selu',kernel_initializer='he_uniform'),\n",
        "                  layers.BatchNormalization(), \n",
        "                  layers.Dense(100,activation='selu',kernel_initializer='he_uniform'),\n",
        "                  layers.BatchNormalization(), \n",
        "                  layers.Dense(100,activation='selu',kernel_initializer='he_uniform'),\n",
        "                  layers.BatchNormalization(), \n",
        "                  layers.Dense(100,activation='selu',kernel_initializer='he_uniform'),\n",
        "                  layers.BatchNormalization(), \n",
        "                  layers.Dense(100,activation='selu',kernel_initializer='he_uniform'),\n",
        "                  layers.BatchNormalization(), \n",
        "                  layers.Dense(100,activation='selu',kernel_initializer='he_uniform'),\n",
        "                  layers.BatchNormalization(), \n",
        "                  layers.Dense(100,activation='selu',kernel_initializer='he_uniform'),\n",
        "                  layers.BatchNormalization(), \n",
        "                  layers.Dense(100,activation='selu',kernel_initializer='he_uniform'),\n",
        "                  layers.BatchNormalization(), \n",
        "                  layers.Dense(100,activation='selu',kernel_initializer='he_uniform'),\n",
        "                  layers.BatchNormalization(), \n",
        "                  layers.Dense(100,activation='selu',kernel_initializer='he_uniform'),\n",
        "                  layers.BatchNormalization(), \n",
        "                  layers.Dense(100,activation='selu',kernel_initializer='he_uniform'),\n",
        "                  layers.BatchNormalization(), \n",
        "                  layers.Dense(100,activation='selu',kernel_initializer='he_uniform'),\n",
        "                  layers.BatchNormalization(), \n",
        "                  layers.Dense(100,activation='selu',kernel_initializer='he_uniform'),\n",
        "                  layers.BatchNormalization(), \n",
        "                  layers.Dense(100,activation='selu',kernel_initializer='he_uniform'),\n",
        "                  layers.BatchNormalization(), \n",
        "                  layers.Dense(100,activation='selu',kernel_initializer='he_uniform'),\n",
        "                  layers.BatchNormalization(), \n",
        "                  layers.Dense(100,activation='selu',kernel_initializer='he_uniform'),\n",
        "                  layers.BatchNormalization(), \n",
        "                  layers.Dense(100,activation='selu',kernel_initializer='he_uniform'),\n",
        "                  layers.BatchNormalization(), \n",
        "                  layers.Dense(100,activation='selu',kernel_initializer='he_uniform'),\n",
        "                  layers.BatchNormalization(), \n",
        "                  layers.Dense(10,activation='softmax')])\n",
        "model3.compile(optimizer='adam',metrics=['accuracy'],loss=['sparse_categorical_crossentropy'])"
      ],
      "metadata": {
        "id": "ZTe27Pru2xYs"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8du2uU7O-Euo",
        "outputId": "8d715e70-01b0-4906-968b-089100127ca0"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_2 (Flatten)         (None, 3072)              0         \n",
            "                                                                 \n",
            " dense_42 (Dense)            (None, 100)               307300    \n",
            "                                                                 \n",
            " batch_normalization_40 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_43 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " batch_normalization_41 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_44 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " batch_normalization_42 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_45 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " batch_normalization_43 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_46 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " batch_normalization_44 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_47 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " batch_normalization_45 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_48 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " batch_normalization_46 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_49 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " batch_normalization_47 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_50 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " batch_normalization_48 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_51 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " batch_normalization_49 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " batch_normalization_50 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " batch_normalization_51 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_54 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " batch_normalization_52 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_55 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " batch_normalization_53 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " batch_normalization_54 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " batch_normalization_55 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " batch_normalization_56 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_59 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " batch_normalization_57 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_60 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " batch_normalization_58 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " batch_normalization_59 (Bat  (None, 100)              400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 508,210\n",
            "Trainable params: 504,210\n",
            "Non-trainable params: 4,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3.fit(X_train,y_train,validation_split=0.2,epochs=150)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "734upz7w-LZp",
        "outputId": "37ae64de-47e6-4de1-9eb1-23c05469c5ce"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "1250/1250 [==============================] - 20s 13ms/step - loss: 1.9273 - accuracy: 0.3087 - val_loss: 1.8287 - val_accuracy: 0.3285\n",
            "Epoch 2/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.7865 - accuracy: 0.3603 - val_loss: 1.8916 - val_accuracy: 0.3428\n",
            "Epoch 3/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.7476 - accuracy: 0.3752 - val_loss: 1.8228 - val_accuracy: 0.3606\n",
            "Epoch 4/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.6997 - accuracy: 0.3917 - val_loss: 1.7966 - val_accuracy: 0.3671\n",
            "Epoch 5/150\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 1.6644 - accuracy: 0.4038 - val_loss: 1.7709 - val_accuracy: 0.3693\n",
            "Epoch 6/150\n",
            "1250/1250 [==============================] - 14s 12ms/step - loss: 1.6303 - accuracy: 0.4176 - val_loss: 1.5881 - val_accuracy: 0.4346\n",
            "Epoch 7/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.5994 - accuracy: 0.4302 - val_loss: 1.6371 - val_accuracy: 0.4153\n",
            "Epoch 8/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.5753 - accuracy: 0.4396 - val_loss: 1.5959 - val_accuracy: 0.4317\n",
            "Epoch 9/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.5495 - accuracy: 0.4494 - val_loss: 1.6458 - val_accuracy: 0.4144\n",
            "Epoch 10/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.5185 - accuracy: 0.4611 - val_loss: 1.6539 - val_accuracy: 0.4073\n",
            "Epoch 11/150\n",
            "1250/1250 [==============================] - 14s 12ms/step - loss: 1.4958 - accuracy: 0.4676 - val_loss: 1.6132 - val_accuracy: 0.4301\n",
            "Epoch 12/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4845 - accuracy: 0.4752 - val_loss: 1.6764 - val_accuracy: 0.4166\n",
            "Epoch 13/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4699 - accuracy: 0.4803 - val_loss: 1.5483 - val_accuracy: 0.4628\n",
            "Epoch 14/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4476 - accuracy: 0.4865 - val_loss: 1.6912 - val_accuracy: 0.4125\n",
            "Epoch 15/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4350 - accuracy: 0.4912 - val_loss: 1.5622 - val_accuracy: 0.4495\n",
            "Epoch 16/150\n",
            "1250/1250 [==============================] - 16s 12ms/step - loss: 1.4261 - accuracy: 0.4994 - val_loss: 1.6133 - val_accuracy: 0.4384\n",
            "Epoch 17/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4134 - accuracy: 0.5005 - val_loss: 1.5479 - val_accuracy: 0.4641\n",
            "Epoch 18/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3918 - accuracy: 0.5078 - val_loss: 1.5048 - val_accuracy: 0.4742\n",
            "Epoch 19/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3874 - accuracy: 0.5099 - val_loss: 2.0480 - val_accuracy: 0.3460\n",
            "Epoch 20/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3702 - accuracy: 0.5177 - val_loss: 1.5923 - val_accuracy: 0.4478\n",
            "Epoch 21/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3541 - accuracy: 0.5205 - val_loss: 1.5611 - val_accuracy: 0.4542\n",
            "Epoch 22/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3446 - accuracy: 0.5272 - val_loss: 1.5972 - val_accuracy: 0.4611\n",
            "Epoch 23/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3344 - accuracy: 0.5287 - val_loss: 1.6210 - val_accuracy: 0.4533\n",
            "Epoch 24/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3272 - accuracy: 0.5321 - val_loss: 1.5495 - val_accuracy: 0.4627\n",
            "Epoch 25/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3112 - accuracy: 0.5361 - val_loss: 1.5408 - val_accuracy: 0.4549\n",
            "Epoch 26/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3080 - accuracy: 0.5396 - val_loss: 1.4881 - val_accuracy: 0.4825\n",
            "Epoch 27/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.2908 - accuracy: 0.5458 - val_loss: 1.4633 - val_accuracy: 0.4949\n",
            "Epoch 28/150\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 1.2864 - accuracy: 0.5494 - val_loss: 1.5282 - val_accuracy: 0.4657\n",
            "Epoch 29/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.2722 - accuracy: 0.5517 - val_loss: 1.5356 - val_accuracy: 0.4731\n",
            "Epoch 30/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.2587 - accuracy: 0.5577 - val_loss: 1.5038 - val_accuracy: 0.4813\n",
            "Epoch 31/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.2510 - accuracy: 0.5616 - val_loss: 1.4762 - val_accuracy: 0.4910\n",
            "Epoch 32/150\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 1.2407 - accuracy: 0.5648 - val_loss: 1.6986 - val_accuracy: 0.4339\n",
            "Epoch 33/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.2283 - accuracy: 0.5703 - val_loss: 1.5151 - val_accuracy: 0.4886\n",
            "Epoch 34/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.2208 - accuracy: 0.5724 - val_loss: 1.6162 - val_accuracy: 0.4518\n",
            "Epoch 35/150\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 1.2148 - accuracy: 0.5727 - val_loss: 1.4680 - val_accuracy: 0.5012\n",
            "Epoch 36/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.2001 - accuracy: 0.5794 - val_loss: 1.4710 - val_accuracy: 0.4962\n",
            "Epoch 37/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.1938 - accuracy: 0.5816 - val_loss: 1.5236 - val_accuracy: 0.4859\n",
            "Epoch 38/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.1884 - accuracy: 0.5832 - val_loss: 1.5768 - val_accuracy: 0.4608\n",
            "Epoch 39/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.1783 - accuracy: 0.5870 - val_loss: 1.6023 - val_accuracy: 0.4618\n",
            "Epoch 40/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.1670 - accuracy: 0.5885 - val_loss: 1.5160 - val_accuracy: 0.4852\n",
            "Epoch 41/150\n",
            "1250/1250 [==============================] - 14s 12ms/step - loss: 1.1652 - accuracy: 0.5895 - val_loss: 1.5146 - val_accuracy: 0.4948\n",
            "Epoch 42/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.1483 - accuracy: 0.5969 - val_loss: 1.5276 - val_accuracy: 0.4951\n",
            "Epoch 43/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.1455 - accuracy: 0.5963 - val_loss: 1.5835 - val_accuracy: 0.4733\n",
            "Epoch 44/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.1449 - accuracy: 0.5974 - val_loss: 1.4899 - val_accuracy: 0.4937\n",
            "Epoch 45/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.1302 - accuracy: 0.6026 - val_loss: 1.4859 - val_accuracy: 0.4976\n",
            "Epoch 46/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.1232 - accuracy: 0.6058 - val_loss: 1.6218 - val_accuracy: 0.4658\n",
            "Epoch 47/150\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 1.1132 - accuracy: 0.6108 - val_loss: 1.4822 - val_accuracy: 0.4996\n",
            "Epoch 48/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.1063 - accuracy: 0.6120 - val_loss: 1.5905 - val_accuracy: 0.4799\n",
            "Epoch 49/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.1051 - accuracy: 0.6099 - val_loss: 1.5118 - val_accuracy: 0.4985\n",
            "Epoch 50/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.0937 - accuracy: 0.6181 - val_loss: 1.4859 - val_accuracy: 0.5015\n",
            "Epoch 51/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.0918 - accuracy: 0.6181 - val_loss: 1.4983 - val_accuracy: 0.4980\n",
            "Epoch 52/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.0804 - accuracy: 0.6211 - val_loss: 1.5673 - val_accuracy: 0.4924\n",
            "Epoch 53/150\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 1.0750 - accuracy: 0.6232 - val_loss: 1.5842 - val_accuracy: 0.4889\n",
            "Epoch 54/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.0697 - accuracy: 0.6273 - val_loss: 1.5616 - val_accuracy: 0.4920\n",
            "Epoch 55/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.0616 - accuracy: 0.6263 - val_loss: 1.5882 - val_accuracy: 0.4726\n",
            "Epoch 56/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.0604 - accuracy: 0.6305 - val_loss: 1.5060 - val_accuracy: 0.4971\n",
            "Epoch 57/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.0513 - accuracy: 0.6291 - val_loss: 1.5825 - val_accuracy: 0.4777\n",
            "Epoch 58/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.0455 - accuracy: 0.6348 - val_loss: 1.6113 - val_accuracy: 0.4735\n",
            "Epoch 59/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.0407 - accuracy: 0.6367 - val_loss: 1.5737 - val_accuracy: 0.4796\n",
            "Epoch 60/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.0344 - accuracy: 0.6355 - val_loss: 1.5538 - val_accuracy: 0.4855\n",
            "Epoch 61/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.0266 - accuracy: 0.6384 - val_loss: 1.4997 - val_accuracy: 0.4986\n",
            "Epoch 62/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.0319 - accuracy: 0.6389 - val_loss: 1.5635 - val_accuracy: 0.4734\n",
            "Epoch 63/150\n",
            "1250/1250 [==============================] - 14s 12ms/step - loss: 1.0235 - accuracy: 0.6400 - val_loss: 1.5550 - val_accuracy: 0.4937\n",
            "Epoch 64/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.0211 - accuracy: 0.6421 - val_loss: 1.6100 - val_accuracy: 0.4773\n",
            "Epoch 65/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.0112 - accuracy: 0.6463 - val_loss: 1.4924 - val_accuracy: 0.5071\n",
            "Epoch 66/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.0144 - accuracy: 0.6451 - val_loss: 1.6697 - val_accuracy: 0.4580\n",
            "Epoch 67/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 1.0038 - accuracy: 0.6486 - val_loss: 1.6045 - val_accuracy: 0.4813\n",
            "Epoch 68/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9933 - accuracy: 0.6522 - val_loss: 1.5846 - val_accuracy: 0.4911\n",
            "Epoch 69/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9918 - accuracy: 0.6531 - val_loss: 1.5737 - val_accuracy: 0.4865\n",
            "Epoch 70/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9858 - accuracy: 0.6555 - val_loss: 1.6132 - val_accuracy: 0.4890\n",
            "Epoch 71/150\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.9756 - accuracy: 0.6570 - val_loss: 1.5826 - val_accuracy: 0.4838\n",
            "Epoch 72/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9758 - accuracy: 0.6592 - val_loss: 1.5349 - val_accuracy: 0.5035\n",
            "Epoch 73/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9675 - accuracy: 0.6608 - val_loss: 1.5841 - val_accuracy: 0.4911\n",
            "Epoch 74/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9709 - accuracy: 0.6591 - val_loss: 1.5432 - val_accuracy: 0.5018\n",
            "Epoch 75/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9646 - accuracy: 0.6623 - val_loss: 1.6057 - val_accuracy: 0.4966\n",
            "Epoch 76/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9514 - accuracy: 0.6664 - val_loss: 1.5380 - val_accuracy: 0.5026\n",
            "Epoch 77/150\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.9485 - accuracy: 0.6694 - val_loss: 1.5697 - val_accuracy: 0.4950\n",
            "Epoch 78/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9485 - accuracy: 0.6692 - val_loss: 1.5491 - val_accuracy: 0.4945\n",
            "Epoch 79/150\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.9504 - accuracy: 0.6667 - val_loss: 1.6591 - val_accuracy: 0.4757\n",
            "Epoch 80/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9320 - accuracy: 0.6750 - val_loss: 1.5773 - val_accuracy: 0.4956\n",
            "Epoch 81/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9326 - accuracy: 0.6728 - val_loss: 1.5559 - val_accuracy: 0.4998\n",
            "Epoch 82/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9305 - accuracy: 0.6761 - val_loss: 1.5774 - val_accuracy: 0.4989\n",
            "Epoch 83/150\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.9253 - accuracy: 0.6784 - val_loss: 1.5716 - val_accuracy: 0.4944\n",
            "Epoch 84/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9188 - accuracy: 0.6786 - val_loss: 1.5483 - val_accuracy: 0.5029\n",
            "Epoch 85/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9155 - accuracy: 0.6798 - val_loss: 1.6236 - val_accuracy: 0.4985\n",
            "Epoch 86/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9132 - accuracy: 0.6812 - val_loss: 1.6096 - val_accuracy: 0.4948\n",
            "Epoch 87/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9045 - accuracy: 0.6810 - val_loss: 1.6739 - val_accuracy: 0.4842\n",
            "Epoch 88/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9063 - accuracy: 0.6838 - val_loss: 1.6277 - val_accuracy: 0.4853\n",
            "Epoch 89/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8994 - accuracy: 0.6835 - val_loss: 1.5853 - val_accuracy: 0.5041\n",
            "Epoch 90/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.9005 - accuracy: 0.6840 - val_loss: 1.6284 - val_accuracy: 0.4812\n",
            "Epoch 91/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8931 - accuracy: 0.6876 - val_loss: 1.6857 - val_accuracy: 0.4920\n",
            "Epoch 92/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8919 - accuracy: 0.6898 - val_loss: 1.7313 - val_accuracy: 0.4944\n",
            "Epoch 93/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8786 - accuracy: 0.6912 - val_loss: 1.6869 - val_accuracy: 0.4879\n",
            "Epoch 94/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8838 - accuracy: 0.6910 - val_loss: 1.6518 - val_accuracy: 0.4826\n",
            "Epoch 95/150\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.8730 - accuracy: 0.6942 - val_loss: 1.6683 - val_accuracy: 0.4963\n",
            "Epoch 96/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8756 - accuracy: 0.6924 - val_loss: 1.6466 - val_accuracy: 0.4938\n",
            "Epoch 97/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8695 - accuracy: 0.6969 - val_loss: 1.6086 - val_accuracy: 0.4939\n",
            "Epoch 98/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8630 - accuracy: 0.6956 - val_loss: 1.6151 - val_accuracy: 0.4909\n",
            "Epoch 99/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8633 - accuracy: 0.7010 - val_loss: 1.6294 - val_accuracy: 0.4999\n",
            "Epoch 100/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8595 - accuracy: 0.6989 - val_loss: 1.6071 - val_accuracy: 0.5067\n",
            "Epoch 101/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8528 - accuracy: 0.7020 - val_loss: 1.7452 - val_accuracy: 0.4754\n",
            "Epoch 102/150\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.8552 - accuracy: 0.7028 - val_loss: 1.6643 - val_accuracy: 0.4896\n",
            "Epoch 103/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8483 - accuracy: 0.7021 - val_loss: 1.7022 - val_accuracy: 0.4950\n",
            "Epoch 104/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8410 - accuracy: 0.7049 - val_loss: 1.6951 - val_accuracy: 0.4910\n",
            "Epoch 105/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8394 - accuracy: 0.7044 - val_loss: 1.7313 - val_accuracy: 0.4847\n",
            "Epoch 106/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8382 - accuracy: 0.7079 - val_loss: 1.6619 - val_accuracy: 0.4967\n",
            "Epoch 107/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8372 - accuracy: 0.7074 - val_loss: 1.7287 - val_accuracy: 0.4788\n",
            "Epoch 108/150\n",
            "1250/1250 [==============================] - 14s 12ms/step - loss: 0.8247 - accuracy: 0.7099 - val_loss: 1.6848 - val_accuracy: 0.4815\n",
            "Epoch 109/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8261 - accuracy: 0.7113 - val_loss: 1.6417 - val_accuracy: 0.4955\n",
            "Epoch 110/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8228 - accuracy: 0.7118 - val_loss: 1.6594 - val_accuracy: 0.4890\n",
            "Epoch 111/150\n",
            "1250/1250 [==============================] - 14s 12ms/step - loss: 0.8285 - accuracy: 0.7110 - val_loss: 1.6306 - val_accuracy: 0.4987\n",
            "Epoch 112/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8176 - accuracy: 0.7144 - val_loss: 1.7000 - val_accuracy: 0.4962\n",
            "Epoch 113/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8130 - accuracy: 0.7153 - val_loss: 1.6644 - val_accuracy: 0.5040\n",
            "Epoch 114/150\n",
            "1250/1250 [==============================] - 14s 12ms/step - loss: 0.8121 - accuracy: 0.7164 - val_loss: 1.7733 - val_accuracy: 0.4751\n",
            "Epoch 115/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8120 - accuracy: 0.7158 - val_loss: 1.7045 - val_accuracy: 0.4851\n",
            "Epoch 116/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7999 - accuracy: 0.7172 - val_loss: 1.7895 - val_accuracy: 0.4696\n",
            "Epoch 117/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8108 - accuracy: 0.7171 - val_loss: 1.6993 - val_accuracy: 0.4891\n",
            "Epoch 118/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8003 - accuracy: 0.7204 - val_loss: 1.6821 - val_accuracy: 0.4924\n",
            "Epoch 119/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8061 - accuracy: 0.7193 - val_loss: 1.7109 - val_accuracy: 0.4966\n",
            "Epoch 120/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7979 - accuracy: 0.7231 - val_loss: 1.6209 - val_accuracy: 0.5079\n",
            "Epoch 121/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.8076 - accuracy: 0.7176 - val_loss: 1.6618 - val_accuracy: 0.4889\n",
            "Epoch 122/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7945 - accuracy: 0.7220 - val_loss: 1.6662 - val_accuracy: 0.5080\n",
            "Epoch 123/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7858 - accuracy: 0.7226 - val_loss: 1.6510 - val_accuracy: 0.5052\n",
            "Epoch 124/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7887 - accuracy: 0.7248 - val_loss: 1.8658 - val_accuracy: 0.4649\n",
            "Epoch 125/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7811 - accuracy: 0.7276 - val_loss: 1.6890 - val_accuracy: 0.4883\n",
            "Epoch 126/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7843 - accuracy: 0.7250 - val_loss: 1.7043 - val_accuracy: 0.4979\n",
            "Epoch 127/150\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.7759 - accuracy: 0.7280 - val_loss: 1.7365 - val_accuracy: 0.4916\n",
            "Epoch 128/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7772 - accuracy: 0.7279 - val_loss: 1.7484 - val_accuracy: 0.4882\n",
            "Epoch 129/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7747 - accuracy: 0.7275 - val_loss: 1.7305 - val_accuracy: 0.4902\n",
            "Epoch 130/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7706 - accuracy: 0.7285 - val_loss: 1.6985 - val_accuracy: 0.4967\n",
            "Epoch 131/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7772 - accuracy: 0.7282 - val_loss: 1.8144 - val_accuracy: 0.4725\n",
            "Epoch 132/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7625 - accuracy: 0.7351 - val_loss: 1.7828 - val_accuracy: 0.4829\n",
            "Epoch 133/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7661 - accuracy: 0.7329 - val_loss: 1.7305 - val_accuracy: 0.4870\n",
            "Epoch 134/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7653 - accuracy: 0.7329 - val_loss: 1.7461 - val_accuracy: 0.4838\n",
            "Epoch 135/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7686 - accuracy: 0.7311 - val_loss: 1.7418 - val_accuracy: 0.4857\n",
            "Epoch 136/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7633 - accuracy: 0.7336 - val_loss: 1.7461 - val_accuracy: 0.4944\n",
            "Epoch 137/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7567 - accuracy: 0.7358 - val_loss: 1.6751 - val_accuracy: 0.5019\n",
            "Epoch 138/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7560 - accuracy: 0.7366 - val_loss: 1.7721 - val_accuracy: 0.4857\n",
            "Epoch 139/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7575 - accuracy: 0.7338 - val_loss: 1.6652 - val_accuracy: 0.4988\n",
            "Epoch 140/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7514 - accuracy: 0.7366 - val_loss: 1.7388 - val_accuracy: 0.4983\n",
            "Epoch 141/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7505 - accuracy: 0.7355 - val_loss: 1.7498 - val_accuracy: 0.4877\n",
            "Epoch 142/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7342 - accuracy: 0.7432 - val_loss: 1.9476 - val_accuracy: 0.4515\n",
            "Epoch 143/150\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.7473 - accuracy: 0.7393 - val_loss: 1.7541 - val_accuracy: 0.4904\n",
            "Epoch 144/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7397 - accuracy: 0.7401 - val_loss: 1.6823 - val_accuracy: 0.5012\n",
            "Epoch 145/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7375 - accuracy: 0.7429 - val_loss: 1.7387 - val_accuracy: 0.4892\n",
            "Epoch 146/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7362 - accuracy: 0.7432 - val_loss: 1.7187 - val_accuracy: 0.4987\n",
            "Epoch 147/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7361 - accuracy: 0.7429 - val_loss: 1.7332 - val_accuracy: 0.4979\n",
            "Epoch 148/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7368 - accuracy: 0.7434 - val_loss: 1.7582 - val_accuracy: 0.4833\n",
            "Epoch 149/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7312 - accuracy: 0.7448 - val_loss: 1.7440 - val_accuracy: 0.4960\n",
            "Epoch 150/150\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.7295 - accuracy: 0.7465 - val_loss: 1.7551 - val_accuracy: 0.4937\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa754ee2220>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Early Stopping implimenation "
      ],
      "metadata": {
        "id": "jJAhHsfO_zX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping"
      ],
      "metadata": {
        "id": "6qMvrTbEAWik"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks =EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    min_delta=0.0001,\n",
        "    patience=20,\n",
        "    verbose=1,\n",
        "    mode=\"auto\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=False\n",
        ")"
      ],
      "metadata": {
        "id": "WLuxETnt-V3I"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.fit(X_train,y_train,validation_split=0.2,epochs=100,callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIUmoygnAmOb",
        "outputId": "f57f5b78-2abe-45e3-c8c9-63c3f6db2861"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6576 - accuracy: 0.7694 - val_loss: 1.9221 - val_accuracy: 0.4809\n",
            "Epoch 2/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6683 - accuracy: 0.7669 - val_loss: 1.8027 - val_accuracy: 0.4958\n",
            "Epoch 3/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6560 - accuracy: 0.7702 - val_loss: 1.8382 - val_accuracy: 0.4868\n",
            "Epoch 4/100\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.6541 - accuracy: 0.7723 - val_loss: 1.8272 - val_accuracy: 0.4911\n",
            "Epoch 5/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6601 - accuracy: 0.7664 - val_loss: 1.8620 - val_accuracy: 0.4977\n",
            "Epoch 6/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6474 - accuracy: 0.7739 - val_loss: 1.8994 - val_accuracy: 0.4898\n",
            "Epoch 7/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6551 - accuracy: 0.7722 - val_loss: 1.9321 - val_accuracy: 0.4816\n",
            "Epoch 8/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6668 - accuracy: 0.7651 - val_loss: 1.8853 - val_accuracy: 0.4879\n",
            "Epoch 9/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6488 - accuracy: 0.7741 - val_loss: 1.8497 - val_accuracy: 0.4919\n",
            "Epoch 10/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6461 - accuracy: 0.7753 - val_loss: 1.8588 - val_accuracy: 0.4841\n",
            "Epoch 11/100\n",
            "1250/1250 [==============================] - 16s 12ms/step - loss: 0.6466 - accuracy: 0.7733 - val_loss: 1.8460 - val_accuracy: 0.4940\n",
            "Epoch 12/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6535 - accuracy: 0.7730 - val_loss: 1.8897 - val_accuracy: 0.4833\n",
            "Epoch 13/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6495 - accuracy: 0.7742 - val_loss: 1.8486 - val_accuracy: 0.4804\n",
            "Epoch 14/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6389 - accuracy: 0.7772 - val_loss: 1.9388 - val_accuracy: 0.4890\n",
            "Epoch 15/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6410 - accuracy: 0.7772 - val_loss: 1.8172 - val_accuracy: 0.5018\n",
            "Epoch 16/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6417 - accuracy: 0.7750 - val_loss: 1.8433 - val_accuracy: 0.4864\n",
            "Epoch 17/100\n",
            "1250/1250 [==============================] - 14s 12ms/step - loss: 0.6391 - accuracy: 0.7778 - val_loss: 1.8518 - val_accuracy: 0.4906\n",
            "Epoch 18/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6301 - accuracy: 0.7819 - val_loss: 1.8620 - val_accuracy: 0.4930\n",
            "Epoch 19/100\n",
            "1250/1250 [==============================] - 14s 12ms/step - loss: 0.6416 - accuracy: 0.7755 - val_loss: 1.9345 - val_accuracy: 0.4800\n",
            "Epoch 20/100\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.6296 - accuracy: 0.7811 - val_loss: 1.9475 - val_accuracy: 0.4736\n",
            "Epoch 21/100\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6376 - accuracy: 0.7781 - val_loss: 1.8937 - val_accuracy: 0.4746\n",
            "Epoch 22/100\n",
            "1250/1250 [==============================] - 14s 12ms/step - loss: 0.6275 - accuracy: 0.7787 - val_loss: 2.0048 - val_accuracy: 0.4780\n",
            "Epoch 22: early stopping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa7d5273a00>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3.fit(X_train,y_train,validation_split=0.2,epochs=22)"
      ],
      "metadata": {
        "id": "GOlFf895A9AU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb23852d-01cd-450a-9436-03a04b5450da"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/22\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.6212 - accuracy: 0.7833 - val_loss: 1.9138 - val_accuracy: 0.4876\n",
            "Epoch 2/22\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6188 - accuracy: 0.7830 - val_loss: 1.8809 - val_accuracy: 0.4806\n",
            "Epoch 3/22\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6287 - accuracy: 0.7797 - val_loss: 1.8412 - val_accuracy: 0.4944\n",
            "Epoch 4/22\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6287 - accuracy: 0.7818 - val_loss: 1.8743 - val_accuracy: 0.4907\n",
            "Epoch 5/22\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6204 - accuracy: 0.7822 - val_loss: 1.8624 - val_accuracy: 0.4929\n",
            "Epoch 6/22\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6227 - accuracy: 0.7862 - val_loss: 1.9169 - val_accuracy: 0.4764\n",
            "Epoch 7/22\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.6182 - accuracy: 0.7853 - val_loss: 2.0211 - val_accuracy: 0.4567\n",
            "Epoch 8/22\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6232 - accuracy: 0.7832 - val_loss: 1.9883 - val_accuracy: 0.4726\n",
            "Epoch 9/22\n",
            "1250/1250 [==============================] - 15s 12ms/step - loss: 0.6154 - accuracy: 0.7841 - val_loss: 1.8899 - val_accuracy: 0.4897\n",
            "Epoch 10/22\n",
            "1250/1250 [==============================] - 16s 12ms/step - loss: 0.6179 - accuracy: 0.7827 - val_loss: 1.9930 - val_accuracy: 0.4785\n",
            "Epoch 11/22\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6161 - accuracy: 0.7829 - val_loss: 1.9928 - val_accuracy: 0.4721\n",
            "Epoch 12/22\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6168 - accuracy: 0.7829 - val_loss: 1.8480 - val_accuracy: 0.4934\n",
            "Epoch 13/22\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6054 - accuracy: 0.7891 - val_loss: 1.8852 - val_accuracy: 0.4927\n",
            "Epoch 14/22\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6077 - accuracy: 0.7894 - val_loss: 2.0356 - val_accuracy: 0.4780\n",
            "Epoch 15/22\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6076 - accuracy: 0.7873 - val_loss: 1.9311 - val_accuracy: 0.4861\n",
            "Epoch 16/22\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6056 - accuracy: 0.7874 - val_loss: 1.9952 - val_accuracy: 0.4802\n",
            "Epoch 17/22\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6105 - accuracy: 0.7868 - val_loss: 1.8910 - val_accuracy: 0.4905\n",
            "Epoch 18/22\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6078 - accuracy: 0.7894 - val_loss: 1.8882 - val_accuracy: 0.4945\n",
            "Epoch 19/22\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6066 - accuracy: 0.7904 - val_loss: 1.9354 - val_accuracy: 0.4816\n",
            "Epoch 20/22\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6029 - accuracy: 0.7891 - val_loss: 1.8452 - val_accuracy: 0.5006\n",
            "Epoch 21/22\n",
            "1250/1250 [==============================] - 14s 11ms/step - loss: 0.6019 - accuracy: 0.7878 - val_loss: 1.8987 - val_accuracy: 0.4883\n",
            "Epoch 22/22\n",
            "1250/1250 [==============================] - 16s 13ms/step - loss: 0.6014 - accuracy: 0.7909 - val_loss: 1.9272 - val_accuracy: 0.4853\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa7d5b55610>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4dm-WRQX0Bl",
        "outputId": "cb44eb63-e666-47ea-9833-3088d56b4104"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 3ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3.46088578e-04, 3.52037558e-03, 4.85729804e-04, ...,\n",
              "        3.12926807e-02, 5.79591274e-01, 3.67795862e-02],\n",
              "       [3.14561836e-02, 1.85580365e-02, 1.37836370e-03, ...,\n",
              "        8.98140832e-04, 4.28155124e-01, 5.16811311e-01],\n",
              "       [7.72634009e-03, 5.81157446e-01, 6.55095035e-04, ...,\n",
              "        6.20151591e-03, 2.98187546e-02, 3.71755123e-01],\n",
              "       ...,\n",
              "       [7.20114913e-03, 7.86046512e-05, 3.41874301e-01, ...,\n",
              "        1.09453695e-02, 4.67316800e-04, 5.86457492e-04],\n",
              "       [2.10516557e-01, 1.84837162e-01, 7.13459328e-02, ...,\n",
              "        1.22411072e-01, 3.87303606e-02, 7.36647174e-02],\n",
              "       [1.58996955e-02, 1.54196714e-05, 3.70883197e-01, ...,\n",
              "        6.62013665e-02, 2.61251815e-03, 4.04848572e-04]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bT9fkf5kaw_V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}