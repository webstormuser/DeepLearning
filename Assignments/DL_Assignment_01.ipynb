{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is the function of a summation junction of a neuron? What is threshold activation\n",
        "function?"
      ],
      "metadata": {
        "id": "XVUoY0fqnYpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS : summation, in physiology, the additive effect of several electrical impulses on a neuromuscular junction, the junction between a nerve cell and a muscle cell. Individually the stimuli cannot evoke a response, but collectively they can generate a response.\n",
        "A threshold activation function (or simply the activation function, also known as squashing function) results in an output signal only when an input signal exceeding a specific threshold value comes as an input"
      ],
      "metadata": {
        "id": "7h5GAN3_nbJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is a step function? What is the difference of step function with threshold function?"
      ],
      "metadata": {
        "id": "AaGuuHrin3pX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS : A step function is a function like that used by the original Perceptron. The output is a certain value, A1, if the input sum is above a certain threshold and A0 if the input sum is below a certain threshold. The values used by the Perceptron were A1 = 1 and A0 = 0.\n",
        "\n",
        "Step Function is one of the simplest kind of activation functions. In this, we consider a threshold value and if the value of net input say y is greater than the threshold then the neuron is activated"
      ],
      "metadata": {
        "id": "cH5QCmVq8Hgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Explain the McCulloch–Pitts model of neuron."
      ],
      "metadata": {
        "id": "rNJ92btFBl_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS : it was the first biological proposed neural model .It was given by The McCulloch Pitt's Model of Neuron is the earliest logical simulation of a biological neuron, developed by Warren McCulloch and Warren Pitts in 1943 and hence, the name McCulloch Pitt’s model.\n",
        "The motivation behind the McCulloh Pitt’s Model is a biological neuron. A biological neuron takes an input signal from the dendrites and after processing it passes onto other connected neurons as the output if the signal is received positively, through axons and synapses. This is the basic working of a biological neuron which is interpreted and mimicked using the McCulloh Pitt’s Model.\n",
        "McCulloch Pitt’s model of neuron is a fairly simple model which consists of some (n) binary inputs with some weight associated with each one of them. An input is known as ‘inhibitory input’ if the weight associated with the input is of negative magnitude and is known as ‘excitatory input’ if the weight associated with the input is of positive magnitude. As the inputs are binary, they can take either of the 2 values, 0 or 1. \n",
        "\n"
      ],
      "metadata": {
        "id": "44ly0oHcBqXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Explain the ADALINE network model."
      ],
      "metadata": {
        "id": "TTi6uTR9ElBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS : Adaptive Linear Neuron (Adaline)\n",
        "\n",
        "Adaline which stands for Adaptive Linear Neuron, is a network having a single linear unit. It was developed by Widrow and Hoff in 1960. Some important points about Adaline are as follows −\n",
        "\n",
        "    It uses bipolar activation function.\n",
        "    Adaline neuron can be trained using Delta rule or Least Mean Square(LMS) rule or widrow-hoff rule\n",
        "    The net input is compared with the target value to compute the error signal.\n",
        "    on the basis of adaptive training algoritham weights are adjusted\n",
        "\n",
        "\n",
        "Adaptive Linear Neuron Learning algorithm\n",
        "\n",
        "Step 0: initialize the weights and the bias are set to some random values but not to zero, also initialize the learning rate α.\n",
        "\n",
        "Step 1 − perform steps 2-7 when stopping condition is false.\n",
        "Step 2 − perform steps 3-5 for each bipolar training pair s:t.\n",
        "\n",
        "Step 3 − Activate each input unit as follows −\n",
        "xi=si(i=1ton)\n",
        "\n",
        "Step 4 − Obtain the net input with the following relation −\n",
        "\n",
        "yin=∑inxi.wi+b\n",
        "\n",
        "\n",
        "Here ‘b’ is bias and ‘n’ is the total number of input neurons.\n",
        "\n",
        "Step 5 Until least mean square is obtained (t - yin), Adjust the weight and bias as follows −\n",
        "\n",
        "      wi(new) = wi(old) + α(t - yin)xi\n",
        "      b(new) = b(old) + α(t - yin)\n",
        "\n",
        "Now calculate the error using => E = (t - yin)2\n",
        "\n",
        "Step 7 − Test for the stopping condition, if error generated is less then or equal to specified tolerance then stop.\n"
      ],
      "metadata": {
        "id": "WrBe1zchEmAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is the constraint of a simple perceptron? Why it may fail with a real-world data set?"
      ],
      "metadata": {
        "id": "0rFvDzpYH9rP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans:The most glaring limitation of the perceptron is the fact that it is only capable of solving classification problems that are linearly separable. This implies separation by a line in two-dimensional space, a plane in three-dimensional space, and a hyperplane in p-dimensional space.\n",
        "A simple perceprton may fail with a real world data set because real world dataset dow not garanty about the linearly seperable of classes."
      ],
      "metadata": {
        "id": "EQW-yoDvIB0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is linearly inseparable problem? What is the role of the hidden layer?"
      ],
      "metadata": {
        "id": "oOwNHDc5Mlqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans :Clearly not all decision problems are linearly separable: they cannot be solved using a linear decision boundary. Problems like these are termed linearly inseparable.\n",
        "\n",
        "  Role of Hidden Layer:\n",
        "    A hidden layer in an artificial neural network is a layer in between input layers and output layers, where artificial neurons take in a set of weighted inputs and produce an output through an activation function.\n",
        "    In short, the hidden layers perform nonlinear transformations of the inputs entered into the network."
      ],
      "metadata": {
        "id": "KomhKg1FMnB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Explain XOR problem in case of a simple perceptron."
      ],
      "metadata": {
        "id": "4B8NjkMOMnL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS :We conclude that a single perceptron with an Heaviside activation function can implement each one of the fundamental logical functions: NOT, AND and OR.\n",
        "They are called fundamental because any logical function, no matter how complex, can be obtained by a combination of those three. We can infer that, if we appropriately connect the three perceptrons we just built, we can implement any logical function"
      ],
      "metadata": {
        "id": "id2DC0ZYNIPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Design a multi-layer perceptron to implement A XOR B."
      ],
      "metadata": {
        "id": "RWv4EQUaRzVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS :In the field of Machine Learning, the Perceptron is a Supervised Learning Algorithm for binary classifiers. \n",
        "Step1: Now for the corresponding weight vector $\\boldsymbol{w} : (\\boldsymbol{w_{1}}, \\boldsymbol{w_{2}})$ of the input vector $\\boldsymbol{x} : (\\boldsymbol{x_{1}}, \\boldsymbol{x_{2}})$ to the AND and OR node\n",
        "\n",
        "Step2: The output ($\\boldsymbol{\\hat{y}}_{1}$) from the AND node will be inputted to the NOT node with weight $\\boldsymbol{w_{NOT}}$ and the associated Perceptron Function can be defined as:\n",
        "\n",
        "Step3: The output ($\\boldsymbol{\\hat{y}}_{2}$) from the OR node and the output ($\\boldsymbol{\\hat{y}}_{3}$) from NOT node as mentioned in Step2 will be inputted to the AND node with weight $(\\boldsymbol{w_{AND1}}, \\boldsymbol{w_{AND2}})$ . Then the corresponding output $\\boldsymbol{\\hat{y}}$ is the final output of the XOR logic function.\n",
        "\n"
      ],
      "metadata": {
        "id": "dEfpni0nRzZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_l1A8FjlRzje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing Python library\n",
        "import numpy as np\n",
        " \n",
        "# define Unit Step Function\n",
        "def unitStep(v):\n",
        "    if v >= 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        " \n",
        "# design Perceptron Model\n",
        "def perceptronModel(x, w, b):\n",
        "    v = np.dot(w, x) + b\n",
        "    y = unitStep(v)\n",
        "    return y\n",
        " \n",
        "# NOT Logic Function\n",
        "# wNOT = -1, bNOT = 0.5\n",
        "def NOT_logicFunction(x):\n",
        "    wNOT = -1\n",
        "    bNOT = 0.5\n",
        "    return perceptronModel(x, wNOT, bNOT)\n",
        " \n",
        "# AND Logic Function\n",
        "# here w1 = wAND1 = 1,\n",
        "# w2 = wAND2 = 1, bAND = -1.5\n",
        "def AND_logicFunction(x):\n",
        "    w = np.array([1, 1])\n",
        "    bAND = -1.5\n",
        "    return perceptronModel(x, w, bAND)\n",
        " \n",
        "# OR Logic Function\n",
        "# w1 = 1, w2 = 1, bOR = -0.5\n",
        "def OR_logicFunction(x):\n",
        "    w = np.array([1, 1])\n",
        "    bOR = -0.5\n",
        "    return perceptronModel(x, w, bOR)\n",
        " \n",
        "# XOR Logic Function\n",
        "# with AND, OR and NOT \n",
        "# function calls in sequence\n",
        "def XOR_logicFunction(x):\n",
        "    y1 = AND_logicFunction(x)\n",
        "    y2 = OR_logicFunction(x)\n",
        "    y3 = NOT_logicFunction(y1)\n",
        "    final_x = np.array([y2, y3])\n",
        "    finalOutput = AND_logicFunction(final_x)\n",
        "    return finalOutput\n",
        " \n",
        "# testing the Perceptron Model\n",
        "test1 = np.array([0, 1])\n",
        "test2 = np.array([1, 1])\n",
        "test3 = np.array([0, 0])\n",
        "test4 = np.array([1, 0])\n",
        " \n",
        "print(\"XOR({}, {}) = {}\".format(0, 1, XOR_logicFunction(test1)))\n",
        "print(\"XOR({}, {}) = {}\".format(1, 1, XOR_logicFunction(test2)))\n",
        "print(\"XOR({}, {}) = {}\".format(0, 0, XOR_logicFunction(test3)))\n",
        "print(\"XOR({}, {}) = {}\".format(1, 0, XOR_logicFunction(test4)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShHkZQDcSI9M",
        "outputId": "67a6a4ae-fad4-4a5d-fe6a-611dcb91f240"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XOR(0, 1) = 1\n",
            "XOR(1, 1) = 0\n",
            "XOR(0, 0) = 0\n",
            "XOR(1, 0) = 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Explain the single-layer feed forward architecture of ANN."
      ],
      "metadata": {
        "id": "m2jQrVr_Sv8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS :A layer is formed by taking a processing element and combining it with other processing elements. When a layer of the processing nodes is formed, the inputs can be connected to these nodes with various weights, resulting in a series of outputs, one per node.This is the simplest type of ANN architecture, where the information flows in one direction from input to output. The layers are fully connected, meaning each neuron in a layer is connected to all the neurons in the next layer."
      ],
      "metadata": {
        "id": "4rkViQq3SwKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Explain the competitive network architecture of ANN."
      ],
      "metadata": {
        "id": "XrywbDQwSwOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS :A competitive network is typically a type of unsupervised machine learning, using the principle of competitive learning to provide results. Through specific mathematical and network modeling, competitive networks achieve various goals in input recognition and processing."
      ],
      "metadata": {
        "id": "W1X9RK8NSwSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Consider a multi-layer feed forward neural network. Enumerate and explain steps in the\n",
        "backpropagation algorithm used to train the network."
      ],
      "metadata": {
        "id": "jToTbY-uUVnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS :Back propagation in Neural Networks\n",
        "  The principle behind the back propagation algorithm is to reduce the    error values in randomly allocated weights and biases such that   it produces the correct output. The system is trained in the supervised  learning method, where the error between the system’s output and a known  expected output is presented to the system and used to modify its internal state. We need to update the weights such that we get the global loss minimum. This is how back propagation in neural networks works.When the gradient is negative, an increase in weight decreases the error.\n",
        "\n",
        "When the gradient is positive, the decrease in weight decreases the error.\n",
        "\n",
        "Working of Back Propagation Algorithm\n",
        "  The goal of back propagation algorithm is to optimize the weights so that the neural network can learn how to correctly map arbitrary inputs to outputs. Here, we will understand the complete scenario of back propagation in neural networks with help of a single training set.\n",
        "  "
      ],
      "metadata": {
        "id": "5Z_-Je_GUWY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What are the advantages and disadvantages of neural networks?"
      ],
      "metadata": {
        "id": "uP6fKDJXW4Ba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS : Advantage of neural network: \n",
        "  ANNs have the ability to learn and model non-linear and complex relationships , which is really important because in real-life, many of the relationships between inputs and outputs are non-linear as well as complex.\n",
        "\n",
        "  Disadvantages of Artificial Neural Networks (ANN)\n",
        "\n",
        "    Hardware dependence:  Artificial neural networks require processors with parallel processing power, by their structure. For this reason, the realization of the equipment is dependent. \n",
        "\n",
        "    Unexplained functioning of the network: This is the most important problem of ANN. When ANN gives a probing solution, it does not give a clue as to why and how. This reduces trust in the network. \n",
        "\n",
        "    Assurance of proper network structure:  There is no specific rule for determining the structure of artificial neural networks. The appropriate network structure is achieved through experience and trial and error. \n",
        "\n",
        "    The difficulty of showing the problem to the network:  ANNs can work with numerical information. Problems have to be translated into numerical values before being introduced to ANN. The display mechanism to be determined here will directly influence the performance of the network. This depends on the user's ability. \n",
        "\n",
        "    The duration of the network is unknown: The network is reduced to a certain value of the error on the sample means that the training has been completed. This value does not give us optimum results. \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AiYHfug8W4Hm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Write short notes on any two of the following:\n",
        "\n",
        "   Biological neuron\n",
        "\n",
        "   ReLU function\n",
        "\n",
        "   Single-layer feed forward ANN\n",
        "\n",
        "   Gradient descent\n",
        "   \n",
        "   Recurrent networks"
      ],
      "metadata": {
        "id": "YgyuHYGAXopZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS : Biological neurons:\n",
        "\n",
        "    Humans have made several attempts to mimic the biological systems, and one of them is artificial neural networks inspired by the biological neural networks in living organisms. However, they are very much different in several ways. For example, the birds had inspired humans to create airplanes, and the four-legged animals inspired us to develop cars.\n",
        "    he artificial counterparts are definitely more powerful and make our life better. The perceptrons, who are the predecessors of artificial neurons, were created to mimic certain parts of a biological neuron such as dendrite, axon, and cell body using mathematical models, electronics, and whatever limited information we have of biological neural networks.\n",
        "    The neuron is the fundamental building block of neural networks. In the biological systems, a neuron is a cell just like any other cell of the body, which has a DNA code and is generated in the same way as the other cells. Though it might have different DNA, the function is similar in all the organisms. A neuron comprises three major parts: the cell body (also called Soma), the dendrites, and the axon. The dendrites are like fibers branched in different directions and are connected to many cells in that cluster. \n",
        "    Dendrites receive the signals from surrounding neurons, and the axon transmits the signal to the other neurons. At the ending terminal of the axon, the contact with the dendrite is made through a synapse. Axon is a long fiber that transports the output signal as electric impulses along its length. Each neuron has one axon. Axons pass impulses from one neuron to another like a domino effect.\n",
        "    "
      ],
      "metadata": {
        "id": "VKJLCjjWXo2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: Gradient Descent:\n",
        "  Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks. Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates.The best way to define the local minimum or local maximum of a function using gradient descent is as follows:\n",
        "\n",
        "    If we move towards a negative gradient or away from the gradient of the function at the current point, it will give the local minimum of that function.\n",
        "    Whenever we move towards a positive gradient or towards the gradient of the function at the current point, we will get the local maximum of that function.\n",
        "This entire procedure is known as Gradient Ascent, which is also known as steepest descent. The main objective of using a gradient descent algorithm is to minimize the cost function using iteration. To achieve this goal, it performs two steps iteratively:\n",
        "\n",
        "    Calculates the first-order derivative of the function to compute the gradient or slope of that function.\n",
        "    Move away from the direction of the gradient, which means slope increased from the current point by alpha times, where Alpha is defined as Learning Rate. It is a tuning parameter in the optimization process which helps to decide the length of the steps.\n"
      ],
      "metadata": {
        "id": "N3f3UjEBW4L_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P1IdYlU2SJr7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}